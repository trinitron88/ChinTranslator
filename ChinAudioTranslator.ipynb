{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu96AxyXurNY",
        "outputId": "237cea96-dd36-49b3-bb25-2e15d32697f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Unzipping ChinTranslator.zip...\n",
            "‚úÖ Done!\n",
            "\n",
            "üìÇ Folder exists: True\n",
            "\n",
            "Contents:\n",
            "  ‚Ä¢ training_data.json\n",
            "  ‚Ä¢ requirements.md\n",
            "  ‚Ä¢ fine-tuning.py\n",
            "  ‚Ä¢ .DS_Store\n",
            "  ‚Ä¢ train-validation.py\n",
            "  ‚Ä¢ SetupWhisper.py\n",
            "  ‚Ä¢ Audio\n",
            "  ‚Ä¢ .venv\n",
            "  ‚Ä¢ Text\n"
          ]
        }
      ],
      "source": [
        "# Unzip the file\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/ChinTranslator.zip'\n",
        "extract_path = '/content/drive/MyDrive/'\n",
        "\n",
        "print(\"üì¶ Unzipping ChinTranslator.zip...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"‚úÖ Done!\")\n",
        "\n",
        "# Verify it worked\n",
        "folder_path = '/content/drive/MyDrive/ChinTranslator'\n",
        "print(f\"\\nüìÇ Folder exists: {os.path.exists(folder_path)}\")\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    print(\"\\nContents:\")\n",
        "    for item in os.listdir(folder_path):\n",
        "        print(f\"  ‚Ä¢ {item}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K_JQGsIyl83",
        "outputId": "e359b35a-3454-4f01-b57b-f99308542419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ChinTranslator\n",
            "/content/drive/MyDrive/ChinTranslator\n",
            "total 114\n",
            "drwx------ 2 root root  4096 Oct 31 00:46 Audio\n",
            "-rw------- 1 root root  6148 Oct 31 00:46 .DS_Store\n",
            "-rw------- 1 root root  6604 Oct 31 00:46 fine-tuning.py\n",
            "-rw------- 1 root root  1985 Oct 31 00:46 requirements.md\n",
            "-rw------- 1 root root  1696 Oct 31 00:46 SetupWhisper.py\n",
            "drwx------ 2 root root  4096 Oct 31 00:46 Text\n",
            "-rw------- 1 root root 84970 Oct 31 00:46 training_data.json\n",
            "-rw------- 1 root root  1237 Oct 31 00:46 train-validation.py\n",
            "drwx------ 6 root root  4096 Oct 31 00:46 .venv\n"
          ]
        }
      ],
      "source": [
        "# Go to your ChinTranslator folder\n",
        "%cd /content/drive/MyDrive/ChinTranslator\n",
        "\n",
        "# Verify you're in the right place\n",
        "!pwd\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9OSlJEuzY1Q",
        "outputId": "38481868-7ae2-4873-cf73-0c225f562b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 16 text files\n",
            "\n",
            "Processing files...\n",
            "\n",
            "‚úì Paired: mark_01.txt ‚Üî mark_01.mp3\n",
            "  Text length: 5567 characters\n",
            "  Text preview: Pathian Fapa Jesuh Khrih kong Thawng·π≠ha cu hitihin aa thawkning a si.\n",
            "Profet Isaiah nih, \"Pathian ni...\n",
            "\n",
            "‚úì Paired: mark_02.txt ‚Üî mark_02.mp3\n",
            "  Text length: 4251 characters\n",
            "  Text preview: Cu hnu ni tlawmpal ah khan Jesuh cu Kapernaum khua ah khan a ra ·π≠han i a inn ah a um ti thawng kha a...\n",
            "\n",
            "‚úì Paired: mark_03.txt ‚Üî mark_03.mp3\n",
            "  Text length: 4176 characters\n",
            "  Text preview: Jesuh cu pumhnak inn ah khan a va kal ·π≠han i cuka ah cun a kut a zengmipa pakhat a rak um. Jesuh cu ...\n",
            "\n",
            "‚úì Paired: mark_04.txt ‚Üî mark_04.mp3\n",
            "  Text length: 5368 characters\n",
            "  Text preview: Jesuh nih Galilee Rili kamah khan a cawnpiak ·π≠han hna. A velchum i aa pummi cu an tam tuk caah lawng...\n",
            "\n",
            "‚úì Paired: mark_05.txt ‚Üî mark_05.mp3\n",
            "  Text length: 5166 characters\n",
            "  Text preview: Cuticun Galilee Rili ral, Gerasa mi hna ram cu an va phan. Jesuh cu lawng chungin a chuah cangka kha...\n",
            "\n",
            "‚úì Paired: mark_06.txt ‚Üî mark_06.mp3\n",
            "  Text length: 943 characters\n",
            "  Text preview: Jesuh nih cuka hmun cun aa thawh i a chuahkehnak khua ah khan a zultu hna he an kal. Sabbath ni ah k...\n",
            "\n",
            "‚úì Paired: mark_07.txt ‚Üî mark_07.mp3\n",
            "  Text length: 5307 characters\n",
            "  Text preview: Farasi mi bu khat le Phungbia cawnpiaktu saya tlawmpal Jerusalem in a rami hna nih khan Jesuh cu an ...\n",
            "\n",
            "‚úì Paired: mark_08.txt ‚Üî mark_08.mp3\n",
            "  Text length: 4903 characters\n",
            "  Text preview: Cu hnu tlawmpal ah cun mi tampi an i pum ·π≠han i ei awk zeihmanh an ngeih ti lo tikah Jesuh nih a zul...\n",
            "\n",
            "‚úì Paired: mark_09.txt ‚Üî mark_09.mp3\n",
            "  Text length: 6518 characters\n",
            "  Text preview: Cun a thawh hna i, \"Biatak kan chimh hna, hika i a dirmi lakah hin, Pathian pennak a ·π≠hawnnak he a r...\n",
            "\n",
            "‚úì Paired: mark_10.txt ‚Üî mark_10.mp3\n",
            "  Text length: 7156 characters\n",
            "  Text preview: Jesuh cu cuka hmun cun a chuak i Judea ram ah khan a kal i Jordan tiva kha a tan. Cun mi tampi nih k...\n",
            "\n",
            "‚úì Paired: mark_11.txt ‚Üî mark_11.mp3\n",
            "  Text length: 4482 characters\n",
            "  Text preview: Jerusalem khua kha an hei naih cang i Oliv Tlang i a ummi Bethfeih le Bethani khua kha an phak tikah...\n",
            "\n",
            "‚úì Paired: mark_12.txt ‚Üî mark_12.mp3\n",
            "  Text length: 6674 characters\n",
            "  Text preview: Cun Jesuh nih tahchunhnak bia in a chimh hna. Aa thawh i, \"Mipa pakhat a rak um i cu pa nih cun mits...\n",
            "\n",
            "‚úì Paired: mark_13.txt ‚Üî mark_13.mp3\n",
            "  Text length: 4869 characters\n",
            "  Text preview: Jesuh cu Biakinn in a kalpah ah khin a zultu pakhat nih a thawh i, \"Saya, zohhmanh, hi lung ·π≠ha·π≠ha h...\n",
            "\n",
            "‚úì Paired: mark_14.txt ‚Üî mark_14.mp3\n",
            "  Text length: 9348 characters\n",
            "  Text preview: Atu cu Lanhtak Puai le Thilnu Cawh Lo Changreu Puai kha ni hnih lawng a duh cang. Tlangbawi upa hna ...\n",
            "\n",
            "‚úì Paired: mark_15.txt ‚Üî mark_15.mp3\n",
            "  Text length: 5347 characters\n",
            "  Text preview: A thaizing zingkate ah tlangbawi upa hna cu haotu upa hna le phungbia cawnpiaktu saya hna le biaceih...\n",
            "\n",
            "‚úì Paired: mark_16.txt ‚Üî mark_16.mp3\n",
            "  Text length: 2352 characters\n",
            "  Text preview: Sabbath ni kha a dih tikah Mary Magdalin le Jeim nu Mary le Salome nih khan Jesuh ruak thuh awk caah...\n",
            "\n",
            "\n",
            "==================================================\n",
            "‚úÖ Dataset created successfully!\n",
            "==================================================\n",
            "Total pairs: 16\n",
            "Saved to: training_data.json\n",
            "\n",
            "Next steps:\n",
            "1. Review the output file to verify everything looks correct\n",
            "2. Check audio file durations match text length expectations\n",
            "3. Ready to start Whisper fine-tuning!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up paths\n",
        "base_dir = Path(\".\")\n",
        "audio_dir = base_dir / \"Audio\"\n",
        "text_dir = base_dir / \"Text\"\n",
        "output_file = base_dir / \"training_data.json\"\n",
        "\n",
        "# Function to read text file\n",
        "def read_text_file(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        return f.read().strip()\n",
        "\n",
        "# Create dataset\n",
        "dataset = []\n",
        "\n",
        "# Get all text files\n",
        "text_files = sorted(text_dir.glob(\"*.txt\"))\n",
        "\n",
        "print(f\"Found {len(text_files)} text files\")\n",
        "print(\"\\nProcessing files...\\n\")\n",
        "\n",
        "for text_file in text_files:\n",
        "    # Get corresponding audio file\n",
        "    audio_file = audio_dir / (text_file.stem + \".mp3\")\n",
        "\n",
        "    # Check if audio file exists\n",
        "    if not audio_file.exists():\n",
        "        print(f\"‚ö†Ô∏è  WARNING: No audio file found for {text_file.name}\")\n",
        "        continue\n",
        "\n",
        "    # Read the text\n",
        "    text = read_text_file(text_file)\n",
        "\n",
        "    # Create data entry\n",
        "    entry = {\n",
        "        \"audio\": str(audio_file),\n",
        "        \"text\": text,\n",
        "        \"chapter\": text_file.stem\n",
        "    }\n",
        "\n",
        "    dataset.append(entry)\n",
        "    print(f\"‚úì Paired: {text_file.name} ‚Üî {audio_file.name}\")\n",
        "    print(f\"  Text length: {len(text)} characters\")\n",
        "    print(f\"  Text preview: {text[:100]}...\")\n",
        "    print()\n",
        "\n",
        "# Save dataset\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"‚úÖ Dataset created successfully!\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Total pairs: {len(dataset)}\")\n",
        "print(f\"Saved to: {output_file}\")\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"1. Review the output file to verify everything looks correct\")\n",
        "print(f\"2. Check audio file durations match text length expectations\")\n",
        "print(f\"3. Ready to start Whisper fine-tuning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riKscxsezL4U",
        "outputId": "78cc7e88-517a-4206-f6ec-bfb2819180a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chapters: 16\n",
            "\n",
            "Train set: 12 chapters\n",
            "Validation set: 4 chapters\n",
            "\n",
            "Train chapters: mark_02, mark_03, mark_06, mark_07, mark_08, mark_09, mark_10, mark_11, mark_13, mark_14, mark_15, mark_16\n",
            "Validation chapters: mark_01, mark_04, mark_05, mark_12\n",
            "\n",
            "‚úÖ Created:\n",
            "  - train_data.json\n",
            "  - val_data.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Load the training data\n",
        "with open('./training_data.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Total chapters: {len(data)}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(data)\n",
        "\n",
        "# Create 80/20 split (13 train, 3 validation)\n",
        "split_idx = int(len(data) * 0.8)\n",
        "train_data = data[:split_idx]\n",
        "val_data = data[split_idx:]\n",
        "\n",
        "print(f\"\\nTrain set: {len(train_data)} chapters\")\n",
        "print(f\"Validation set: {len(val_data)} chapters\")\n",
        "\n",
        "# Show which chapters are in each set\n",
        "train_chapters = [item['chapter'] for item in train_data]\n",
        "val_chapters = [item['chapter'] for item in val_data]\n",
        "\n",
        "print(f\"\\nTrain chapters: {', '.join(sorted(train_chapters))}\")\n",
        "print(f\"Validation chapters: {', '.join(sorted(val_chapters))}\")\n",
        "\n",
        "# Save the splits\n",
        "output_dir = Path('.')\n",
        "\n",
        "with open(output_dir / 'train_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(output_dir / 'val_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Created:\")\n",
        "print(f\"  - {output_dir / 'train_data.json'}\")\n",
        "print(f\"  - {output_dir / 'val_data.json'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f9LjBdc0qYC"
      },
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "!pip install -q transformers datasets accelerate evaluate jiwer librosa soundfile tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOrIBy4Q1JQO"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjrDxGqk2NnN"
      },
      "outputs": [],
      "source": [
        "!pip install -q librosa soundfile torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714,
          "referenced_widgets": [
            "c86c938b130146afb19370d08fb4285c",
            "f88c0f81c0e7449cb3ace76babd0bccf",
            "d41a8dcb586a44ed962bdbd942c6ac5b",
            "dd27452cf7a14c7381236044ee907689",
            "3a92b00ab0a54786bdd95813b6d5c4fd",
            "bcff0f34d34a4d89bc35662444f6c2b1",
            "b3644da5401a44a69e92b783185e2768",
            "3b54c33da1434476ba1535cfcfc8101b",
            "a0a3c11ca39b4515b5f772b82354e6ce",
            "6bd46a587b1e4a71a876a0538917a170",
            "81af122f09b24610b02102dfe7944d5b",
            "b0f1e7e690a24cb79ebae8a9c8115843",
            "b76074a16cc94ae6bbd64e929d77ce87",
            "0603c8530e49487ea08a62ab6f2c6d1a",
            "2684a100a5b143e699647b0783b78974",
            "9c4f6708e550496a9c18c57847757be9",
            "61c3a6d3dc5749b69bb0d0ebf61f9746",
            "10b41ad6c87642c082d8039d3d08fe9f",
            "df219834e198404d9556f92b2166a364",
            "0c8c4c97676e4a55a1ad6214e6b18155",
            "a94d7b3ca7dd4542ae76a3360c3057c4",
            "806ad090ff524e398f08a2746077c30f",
            "43ecc78c5dac4a0b94d808fe6ea6735f",
            "8bf0003d76e944488886ef420b39d675",
            "41b1b62a45cd4600be107592188704f7",
            "d531d9f9f08b4b48b92b502506b18a36",
            "867b9b315f8e46b0949969136a8fb242",
            "7be2b8033d2e486392502af5e9cb1afa",
            "ee1c8db8a77a4892a0d798a283d4dc7f",
            "fe5a50a7103549e99f69a3adf9ad9288",
            "86ad791923f246f09cf2e0dd4a3acb6f",
            "60d9b7e61c544ea4996f5b905e26d062",
            "e02f11febfa542828f5fd0137aaa6ff5"
          ]
        },
        "id": "UgBdBGN3zysN",
        "outputId": "dd0a5cc6-f358-43f8-a111-f64b1bf9ef22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Whisper Fine-tuning for Hakha Chin\n",
            "==================================================\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "üìÇ Loading data...\n",
            "Train samples: 12\n",
            "Validation samples: 4\n",
            "‚úÖ Datasets prepared\n",
            "\n",
            "ü§ñ Loading Whisper model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model loaded\n",
            "\n",
            "üîÑ Processing datasets...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c86c938b130146afb19370d08fb4285c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2077 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0f1e7e690a24cb79ebae8a9c8115843",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Datasets processed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43ecc78c5dac4a0b94d808fe6ea6735f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2539840359.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# Training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m training_args = Seq2SeqTrainingArguments(\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Adjust based on GPU memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Whisper Fine-tuning Script for Hakha Chin\n",
        "Run this in Google Colab or on a machine with GPU\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datasets import Dataset, Audio\n",
        "from transformers import (\n",
        "    WhisperFeatureExtractor,\n",
        "    WhisperTokenizer,\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    TrainerCallback\n",
        ")\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import evaluate\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"openai/whisper-small\"  # Start with small model\n",
        "LANGUAGE = None  # Hakha Chin ISO code\n",
        "TASK = \"transcribe\"\n",
        "OUTPUT_DIR = \"./whisper-hakha-chin\"\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Whisper Fine-tuning for Hakha Chin\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check for GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if device == \"cpu\":\n",
        "    print(\"‚ö†Ô∏è  WARNING: No GPU detected. Training will be very slow!\")\n",
        "    print(\"   Consider using Google Colab with GPU enabled.\")\n",
        "\n",
        "# Load the data\n",
        "print(\"\\nüìÇ Loading data...\")\n",
        "with open('./train_data.json', 'r', encoding='utf-8') as f:\n",
        "    train_data = json.load(f)\n",
        "with open('./val_data.json', 'r', encoding='utf-8') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "# Convert to HuggingFace Dataset format\n",
        "def prepare_dataset(data_list):\n",
        "    return Dataset.from_dict({\n",
        "        \"audio\": [item[\"audio\"] for item in data_list],\n",
        "        \"text\": [item[\"text\"] for item in data_list]\n",
        "    })\n",
        "\n",
        "train_dataset = prepare_dataset(train_data)\n",
        "val_dataset = prepare_dataset(val_data)\n",
        "\n",
        "# Cast audio column to Audio feature\n",
        "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print(\"‚úÖ Datasets prepared\")\n",
        "\n",
        "# Load Whisper model components\n",
        "print(\"\\nü§ñ Loading Whisper model...\")\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"‚úÖ Model loaded\")\n",
        "\n",
        "# Prepare data for training\n",
        "def prepare_data(batch):\n",
        "    # Load audio\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # Compute input features\n",
        "    batch[\"input_features\"] = feature_extractor(\n",
        "        audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"]\n",
        "    ).input_features[0]\n",
        "\n",
        "    # Encode target text\n",
        "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "print(\"\\nüîÑ Processing datasets...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    prepare_data,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "val_dataset = val_dataset.map(\n",
        "    prepare_data,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "print(\"‚úÖ Datasets processed\")\n",
        "\n",
        "# Data collator\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Split into input features and labels\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # Pad input features\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Pad labels\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Replace padding with -100 (ignore in loss)\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        # Remove BOS token if present\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "# Evaluation metric\n",
        "metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Replace -100 with pad token\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Compute WER\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
        "    gradient_accumulation_steps=2,  # Effective batch size = 8\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=10,  # Adjust as needed\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True if device == \"cuda\" else False,  # Mixed precision training\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Custom callback for progress\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            print(f\"Step {state.global_step}: {logs}\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[ProgressCallback()],\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(f\"üìÅ Model saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "# Save final model\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "processor.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"\\nüéâ All done! Your fine-tuned Hakha Chin Whisper model is ready!\")\n",
        "print(f\"\\nTo use it:\")\n",
        "print(f\"  model = WhisperForConditionalGeneration.from_pretrained('{OUTPUT_DIR}')\")\n",
        "print(f\"  processor = WhisperProcessor.from_pretrained('{OUTPUT_DIR}')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAHL7wSW2-cd",
        "outputId": "beefc7b6-fd9a-4526-e9d8-858229a18229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ChinTranslator\n",
            "total 201\n",
            "drwx------ 2 root root  4096 Oct 31 00:46 Audio\n",
            "-rw------- 1 root root  6148 Oct 31 00:46 .DS_Store\n",
            "-rw------- 1 root root  6604 Oct 31 01:22 fine-tuning.py\n",
            "drwx------ 2 root root  4096 Oct 31 01:05 .ipynb_checkpoints\n",
            "-rw------- 1 root root  1985 Oct 31 00:46 requirements.md\n",
            "-rw------- 1 root root  1696 Oct 31 01:07 SetupWhisper.py\n",
            "drwx------ 2 root root  4096 Oct 31 00:46 Text\n",
            "-rw------- 1 root root 61554 Oct 31 01:08 train_data.json\n",
            "-rw------- 1 root root 84970 Oct 31 01:08 training_data.json\n",
            "-rw------- 1 root root  1237 Oct 31 01:06 train-validation.py\n",
            "-rw------- 1 root root 23418 Oct 31 01:08 val_data.json\n",
            "drwx------ 6 root root  4096 Oct 31 00:46 .venv\n",
            "2025-10-31 01:22:48.703886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761873768.736143   14312 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761873768.746203   14312 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761873768.769944   14312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761873768.769977   14312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761873768.769986   14312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761873768.769998   14312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "==================================================\n",
            "Whisper Fine-tuning for Hakha Chin\n",
            "==================================================\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "üìÇ Loading data...\n",
            "Train samples: 12\n",
            "Validation samples: 4\n",
            "‚úÖ Datasets prepared\n",
            "\n",
            "ü§ñ Loading Whisper model...\n",
            "‚úÖ Model loaded\n",
            "\n",
            "üîÑ Processing datasets...\n",
            "Map:   0% 0/12 [00:16<?, ? examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ChinTranslator/fine-tuning.py\", line 98, in <module>\n",
            "    train_dataset = train_dataset.map(\n",
            "                    ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 592, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 557, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3097, in map\n",
            "    for rank, done, content in Dataset._map_single(**dataset_kwargs):\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3450, in _map_single\n",
            "    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3353, in apply_function_on_filtered_inputs\n",
            "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ChinTranslator/fine-tuning.py\", line 93, in prepare_data\n",
            "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\", line 2938, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\", line 3048, in _call_one\n",
            "    return self.encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\", line 3123, in encode_plus\n",
            "    return self._encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py\", line 803, in _encode_plus\n",
            "    return self.prepare_for_model(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\", line 3583, in prepare_for_model\n",
            "    total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py\", line 619, in num_special_tokens_to_add\n",
            "    return len(self.build_inputs_with_special_tokens(token_ids_0, token_ids_1 if pair else None))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/tokenization_whisper.py\", line 444, in build_inputs_with_special_tokens\n",
            "    return self.prefix_tokens + token_ids_0 + [self.eos_token_id]\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/tokenization_whisper.py\", line 422, in prefix_tokens\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported language: cnh. Language should be one of: ['english', 'chinese', 'german', 'spanish', 'russian', 'korean', 'french', 'japanese', 'portuguese', 'turkish', 'polish', 'catalan', 'dutch', 'arabic', 'swedish', 'italian', 'indonesian', 'hindi', 'finnish', 'vietnamese', 'hebrew', 'ukrainian', 'greek', 'malay', 'czech', 'romanian', 'danish', 'hungarian', 'tamil', 'norwegian', 'thai', 'urdu', 'croatian', 'bulgarian', 'lithuanian', 'latin', 'maori', 'malayalam', 'welsh', 'slovak', 'telugu', 'persian', 'latvian', 'bengali', 'serbian', 'azerbaijani', 'slovenian', 'kannada', 'estonian', 'macedonian', 'breton', 'basque', 'icelandic', 'armenian', 'nepali', 'mongolian', 'bosnian', 'kazakh', 'albanian', 'swahili', 'galician', 'marathi', 'punjabi', 'sinhala', 'khmer', 'shona', 'yoruba', 'somali', 'afrikaans', 'occitan', 'georgian', 'belarusian', 'tajik', 'sindhi', 'gujarati', 'amharic', 'yiddish', 'lao', 'uzbek', 'faroese', 'haitian creole', 'pashto', 'turkmen', 'nynorsk', 'maltese', 'sanskrit', 'luxembourgish', 'myanmar', 'tibetan', 'tagalog', 'malagasy', 'assamese', 'tatar', 'hawaiian', 'lingala', 'hausa', 'bashkir', 'javanese', 'sundanese', 'cantonese', 'burmese', 'valencian', 'flemish', 'haitian', 'letzeburgesch', 'pushto', 'panjabi', 'moldavian', 'moldovan', 'sinhalese', 'castilian', 'mandarin'].\n"
          ]
        }
      ],
      "source": [
        "# Navigate back to your project folder\n",
        "%cd /content/drive/MyDrive/ChinTranslator\n",
        "\n",
        "# Verify you're in the right place\n",
        "!ls -la\n",
        "\n",
        "# Now run training\n",
        "!python fine-tuning.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ab_sL0br2e0t",
        "outputId": "272e9926-0eeb-4e89-b13e-94a347cf660c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "Collecting datasets==2.14.0\n",
            "  Downloading datasets-2.14.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (18.1.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.0)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.14.0) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.14.0) (6.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.14.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.14.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.14.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.14.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.14.0) (1.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.14.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.14.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.14.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.14.0) (2025.10.5)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.14.0)\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.14.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.14.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.14.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.0) (1.17.0)\n",
            "Downloading datasets-2.14.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill, multiprocess, datasets\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "Successfully installed datasets-2.14.0 dill-0.3.7 multiprocess-0.70.15\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "09855457ca5f43f4b35b3ccc1458ada4",
              "pip_warning": {
                "packages": [
                  "_multiprocess",
                  "datasets",
                  "dill",
                  "multiprocess"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Uninstall and reinstall with specific versions that work together\n",
        "!pip uninstall -y datasets\n",
        "!pip install datasets==2.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893,
          "referenced_widgets": [
            "6f8e1390693b4eaabda78ec4408eafcc",
            "267b4159b646403aadc194f96937050e",
            "f2a8f68baa2b41e0bfab543c20fe0a6b",
            "5b928a8d77164cf09af1d71f6c0dc270",
            "6e68160adef145dd837b4d7f2fe4e021",
            "846f0d16e25444928198678d4dd1109b",
            "b43e4b6ffc6645d0bf857004b8cdad9b",
            "8b6d105924a54756970d20ca946371e2",
            "2600caeb992346d5a184dd8c3ae141d6",
            "85cd416cc10f42b7bb326d7d9b6fb6f3",
            "892b6f1614af4a12ad5764170eb8c8af",
            "349b46601a3244debb5fc1778a9062d1",
            "250b0b1089574afdbb26e70178917fa5",
            "a4fcf85ccec340ad9fb51800bc88ac8a",
            "22acb4513cef48d4852d1f1ae93d5215",
            "abb51b60fe3f4dc6b30b1205d46eb004",
            "64fae0c9a6ff4bd7b997931ca1620065",
            "46815fd171f6491690d0fbffa5d19d99",
            "59c49ad3173e4b20aa2018ce5923bc1e",
            "ad621c0eec8d4b2292d2c1a101ea534b",
            "c35b42c8121c43129908ae58436a7b62",
            "25cacd33f52f474da64e1c3480edc7fc"
          ]
        },
        "id": "h36bpCVj4H7c",
        "outputId": "a55e2f81-e4ff-4867-cc8d-007718834b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Whisper Fine-tuning for Hakha Chin\n",
            "==================================================\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "üìÇ Loading data...\n",
            "Train samples: 12\n",
            "Validation samples: 4\n",
            "‚úÖ Datasets prepared\n",
            "\n",
            "ü§ñ Loading Whisper model...\n",
            "‚úÖ Model loaded\n",
            "\n",
            "üîÑ Processing datasets...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f8e1390693b4eaabda78ec4408eafcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2077 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "349b46601a3244debb5fc1778a9062d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Datasets processed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-436597271.py:188: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Starting training...\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Labels' sequence length 3031 cannot exceed the maximum allowed length of 448 tokens.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-436597271.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m# Train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚úÖ Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_target_positions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1282\u001b[0m                     \u001b[0;34mf\"Labels' sequence length {labels.shape[1]} cannot exceed the maximum allowed length of {self.max_target_positions} tokens.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: Labels' sequence length 3031 cannot exceed the maximum allowed length of 448 tokens."
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Whisper Fine-tuning Script for Hakha Chin\n",
        "Run this in Google Colab or on a machine with GPU\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datasets import Dataset, Audio\n",
        "from transformers import (\n",
        "    WhisperFeatureExtractor,\n",
        "    WhisperTokenizer,\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    TrainerCallback\n",
        ")\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import evaluate\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"openai/whisper-small\"  # Start with small model\n",
        "LANGUAGE = None  # Don't specify language - let model learn Hakha Chin\n",
        "TASK = \"transcribe\"\n",
        "OUTPUT_DIR = \"./whisper-hakha-chin\"\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Whisper Fine-tuning for Hakha Chin\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check for GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if device == \"cpu\":\n",
        "    print(\"‚ö†Ô∏è  WARNING: No GPU detected. Training will be very slow!\")\n",
        "    print(\"   Consider using Google Colab with GPU enabled.\")\n",
        "\n",
        "# Load the data\n",
        "print(\"\\nüìÇ Loading data...\")\n",
        "with open('./train_data.json', 'r', encoding='utf-8') as f:\n",
        "    train_data = json.load(f)\n",
        "with open('./val_data.json', 'r', encoding='utf-8') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "# Convert to HuggingFace Dataset format\n",
        "def prepare_dataset(data_list):\n",
        "    return Dataset.from_dict({\n",
        "        \"audio\": [item[\"audio\"] for item in data_list],\n",
        "        \"text\": [item[\"text\"] for item in data_list]\n",
        "    })\n",
        "\n",
        "train_dataset = prepare_dataset(train_data)\n",
        "val_dataset = prepare_dataset(val_data)\n",
        "\n",
        "# Cast audio column to Audio feature\n",
        "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print(\"‚úÖ Datasets prepared\")\n",
        "\n",
        "# Load Whisper model components\n",
        "print(\"\\nü§ñ Loading Whisper model...\")\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, task=TASK)\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME, task=TASK)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"‚úÖ Model loaded\")\n",
        "\n",
        "# Prepare data for training\n",
        "def prepare_data(batch):\n",
        "    # Load audio\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # Compute input features\n",
        "    batch[\"input_features\"] = feature_extractor(\n",
        "        audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"]\n",
        "    ).input_features[0]\n",
        "\n",
        "    # Encode target text\n",
        "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "print(\"\\nüîÑ Processing datasets...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    prepare_data,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "val_dataset = val_dataset.map(\n",
        "    prepare_data,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "print(\"‚úÖ Datasets processed\")\n",
        "\n",
        "# Data collator\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Split into input features and labels\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # Pad input features\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Pad labels\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Replace padding with -100 (ignore in loss)\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        # Remove BOS token if present\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "# Evaluation metric\n",
        "metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Replace -100 with pad token\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Compute WER\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
        "    gradient_accumulation_steps=2,  # Effective batch size = 8\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=10,  # Adjust as needed\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True if device == \"cuda\" else False,  # Mixed precision training\n",
        "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
        "    per_device_eval_batch_size=2,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Custom callback for progress\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            print(f\"Step {state.global_step}: {logs}\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[ProgressCallback()],\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(f\"üìÅ Model saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "# Save final model\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "processor.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"\\nüéâ All done! Your fine-tuned Hakha Chin Whisper model is ready!\")\n",
        "print(f\"\\nTo use it:\")\n",
        "print(f\"  model = WhisperForConditionalGeneration.from_pretrained('{OUTPUT_DIR}')\")\n",
        "print(f\"  processor = WhisperProcessor.from_pretrained('{OUTPUT_DIR}')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OHKfWG37Vnb",
        "outputId": "1e98258b-f883-4207-e17f-6449c1f70761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYo-1eLm9et7",
        "outputId": "c85303e5-b5a6-446c-b0e4-30dadda3c872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Creating Aligned Training Data\n",
            "==================================================\n",
            "\n",
            "üì• Loading Whisper model...\n",
            "‚úÖ Model loaded\n",
            "\n",
            "üìÇ Loading original data...\n",
            "Found 16 chapters\n",
            "\n",
            "üîÑ Processing mark_01...\n",
            "Detected language: Indonesian\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 37722/40722 [00:55<00:04, 674.98frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 130 segments\n",
            "  Created 54 aligned segments\n",
            "\n",
            "üîÑ Processing mark_02...\n",
            "Detected language: Thai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|‚ñç         | 1221/31221 [01:10<28:40, 17.44frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 3 segments\n",
            "  Created 3 aligned segments\n",
            "\n",
            "üîÑ Processing mark_03...\n",
            "Detected language: Myanmar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31657/31657 [00:15<00:00, 1985.78frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 227 segments\n",
            "  Created 36 aligned segments\n",
            "\n",
            "üîÑ Processing mark_04...\n",
            "Detected language: Armenian\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39920/39920 [00:37<00:00, 1070.59frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 148 segments\n",
            "  Created 51 aligned segments\n",
            "\n",
            "üîÑ Processing mark_05...\n",
            "Detected language: Myanmar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38995/38995 [00:15<00:00, 2486.48frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 170 segments\n",
            "  Created 52 aligned segments\n",
            "\n",
            "üîÑ Processing mark_06...\n",
            "Detected language: Thai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|‚ñå         | 3191/60191 [02:10<38:51, 24.45frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 4 segments\n",
            "  Created 4 aligned segments\n",
            "\n",
            "üîÑ Processing mark_07...\n",
            "Detected language: Khmer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40377/40377 [00:15<00:00, 2611.16frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 78 segments\n",
            "  Created 42 aligned segments\n",
            "\n",
            "üîÑ Processing mark_08...\n",
            "Detected language: Khmer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38023/38023 [00:31<00:00, 1217.22frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 179 segments\n",
            "  Created 44 aligned segments\n",
            "\n",
            "üîÑ Processing mark_09...\n",
            "Detected language: Thai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 424/51424 [01:50<3:41:33,  3.84frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 1 segments\n",
            "  Created 1 aligned segments\n",
            "\n",
            "üîÑ Processing mark_10...\n",
            "Detected language: Thai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 24396/57396 [02:25<03:16, 168.12frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 83 segments\n",
            "  Created 66 aligned segments\n",
            "\n",
            "üîÑ Processing mark_11...\n",
            "Detected language: Khmer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 32523/35523 [00:20<00:01, 1553.32frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 143 segments\n",
            "  Created 43 aligned segments\n",
            "\n",
            "üîÑ Processing mark_12...\n",
            "Detected language: Armenian\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46581/52581 [00:20<00:02, 2304.82frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 79 segments\n",
            "  Created 62 aligned segments\n",
            "\n",
            "üîÑ Processing mark_13...\n",
            "Detected language: Danish\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 32492/35492 [00:32<00:02, 1010.18frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 212 segments\n",
            "  Created 48 aligned segments\n",
            "\n",
            "üîÑ Processing mark_14...\n",
            "Detected language: Myanmar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 41772/74772 [02:06<01:40, 329.86frames/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 265 segments\n",
            "  Created 101 aligned segments\n",
            "\n",
            "üîÑ Processing mark_15...\n",
            "Detected language: Myanmar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46769/46769 [00:17<00:00, 2740.47frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 91 segments\n",
            "  Created 60 aligned segments\n",
            "\n",
            "üîÑ Processing mark_16...\n",
            "Detected language: Thai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|‚ñà‚ñã        | 3504/21504 [00:49<04:14, 70.84frames/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Found 9 segments\n",
            "  Created 9 aligned segments\n",
            "\n",
            "‚úÖ Total aligned segments: 676\n",
            "\n",
            "üìã Sample aligned segment:\n",
            "  Chapter: mark_01\n",
            "  Time: 0.00s - 4.00s\n",
            "  Text: Pathian Fapa Jesuh Khrih kong Thawng·π≠ha cu hitihin aa thawkning a si...\n",
            "\n",
            "üíæ Saved to: aligned_training_data.json\n",
            "\n",
            "Next steps:\n",
            "1. Review the aligned data\n",
            "2. If it looks good, process all 16 chapters\n",
            "3. Split into train/val\n",
            "4. Start fine-tuning!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Use Whisper to get timestamps and align with Hakha Chin text\n",
        "This creates properly chunked training data\n",
        "\"\"\"\n",
        "\n",
        "import whisper\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Creating Aligned Training Data\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load pre-trained Whisper model (tiny for speed)\n",
        "print(\"\\nüì• Loading Whisper model...\")\n",
        "model = whisper.load_model(\"tiny\")\n",
        "print(\"‚úÖ Model loaded\")\n",
        "\n",
        "# Load your original data\n",
        "print(\"\\nüìÇ Loading original data...\")\n",
        "with open('training_data.json', 'r', encoding='utf-8') as f:\n",
        "    original_data = json.load(f)\n",
        "\n",
        "print(f\"Found {len(original_data)} chapters\")\n",
        "\n",
        "# Process each chapter\n",
        "aligned_segments = []\n",
        "\n",
        "for idx, chapter in enumerate(original_data):  # Start with first 3 chapters as test\n",
        "    print(f\"\\nüîÑ Processing {chapter['chapter']}...\")\n",
        "\n",
        "    audio_path = chapter['audio']\n",
        "    full_text = chapter['text']\n",
        "\n",
        "    # Transcribe with timestamps\n",
        "    result = model.transcribe(\n",
        "        audio_path,\n",
        "        language=None,  # Let it detect\n",
        "        task=\"transcribe\",\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    print(f\"  Found {len(result['segments'])} segments\")\n",
        "\n",
        "    # Split text into sentences (rough split by periods)\n",
        "    sentences = [s.strip() for s in full_text.split('.') if s.strip()]\n",
        "\n",
        "    # Match Whisper segments with sentences\n",
        "    # Simple approach: distribute sentences across segments\n",
        "    if len(sentences) > 0:\n",
        "        sentences_per_segment = max(1, len(sentences) // len(result['segments']))\n",
        "\n",
        "        sentence_idx = 0\n",
        "        for seg in result['segments']:\n",
        "            # Take a few sentences for this segment\n",
        "            end_idx = min(sentence_idx + sentences_per_segment, len(sentences))\n",
        "            segment_text = '. '.join(sentences[sentence_idx:end_idx])\n",
        "\n",
        "            if segment_text:  # Only add if we have text\n",
        "                aligned_segments.append({\n",
        "                    'audio': audio_path,\n",
        "                    'start': seg['start'],\n",
        "                    'end': seg['end'],\n",
        "                    'text': segment_text,\n",
        "                    'chapter': chapter['chapter']\n",
        "                })\n",
        "\n",
        "            sentence_idx = end_idx\n",
        "\n",
        "            if sentence_idx >= len(sentences):\n",
        "                break\n",
        "\n",
        "    print(f\"  Created {len(aligned_segments) - len([s for s in aligned_segments if s['chapter'] != chapter['chapter']])} aligned segments\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total aligned segments: {len(aligned_segments)}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nüìã Sample aligned segment:\")\n",
        "if aligned_segments:\n",
        "    sample = aligned_segments[0]\n",
        "    print(f\"  Chapter: {sample['chapter']}\")\n",
        "    print(f\"  Time: {sample['start']:.2f}s - {sample['end']:.2f}s\")\n",
        "    print(f\"  Text: {sample['text'][:100]}...\")\n",
        "\n",
        "# Save aligned data\n",
        "output_file = 'aligned_training_data.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(aligned_segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Saved to: {output_file}\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Review the aligned data\")\n",
        "print(\"2. If it looks good, process all 16 chapters\")\n",
        "print(\"3. Split into train/val\")\n",
        "print(\"4. Start fine-tuning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sAJwzGCB4ZR",
        "outputId": "3bf85543-d91b-4cf1-e0e8-054cf4f2dfe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 540 segments\n",
            "Val: 136 segments\n",
            "\n",
            "‚úÖ Ready for training!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Load aligned data\n",
        "with open('aligned_training_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Shuffle\n",
        "random.seed(42)\n",
        "random.shuffle(data)\n",
        "\n",
        "# 80/20 split\n",
        "split_idx = int(len(data) * 0.8)\n",
        "train_data = data[:split_idx]\n",
        "val_data = data[split_idx:]\n",
        "\n",
        "# Save\n",
        "with open('aligned_train_data.json', 'w') as f:\n",
        "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open('aligned_val_data.json', 'w') as f:\n",
        "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Train: {len(train_data)} segments\")\n",
        "print(f\"Val: {len(val_data)} segments\")\n",
        "print(\"\\n‚úÖ Ready for training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mePIbThSEfIr",
        "outputId": "1ca8847c-ec8d-43a6-d3bb-36c0d1a32d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "COLAB PROJECT FILES\n",
            "============================================================\n",
            "\n",
            "Current Directory: /content/drive/MyDrive/ChinTranslator\n",
            "\n",
            "FILE STRUCTURE:\n",
            "------------------------------------------------------------\n",
            "üìÅ ChinTranslator/\n",
            "  üìÑ SetupWhisper.py (1.7 KB)\n",
            "  üìÑ aligned_train_data.json (130.9 KB)\n",
            "  üìÑ aligned_training_data.json (160.5 KB)\n",
            "  üìÑ aligned_val_data.json (29.7 KB)\n",
            "  üìÑ fine-tuning.py (6.5 KB)\n",
            "  üìÑ requirements.md (1.9 KB)\n",
            "  üìÑ split-aligned-data.py (618.0 B)\n",
            "  üìÑ train-validation.py (1.2 KB)\n",
            "  üìÑ train_data.json (60.1 KB)\n",
            "  üìÑ training_data.json (83.0 KB)\n",
            "  üìÑ val_data.json (22.9 KB)\n",
            "  üìÑ whisper_alignment.py (3.0 KB)\n",
            "  üìÅ Audio/\n",
            "    üìÑ mark_01.mp3 (3.1 MB)\n",
            "    üìÑ mark_02.mp3 (2.4 MB)\n",
            "    üìÑ mark_03.mp3 (2.4 MB)\n",
            "    üìÑ mark_04.mp3 (3.1 MB)\n",
            "    üìÑ mark_05.mp3 (3.0 MB)\n",
            "    üìÑ mark_06.mp3 (4.6 MB)\n",
            "    üìÑ mark_07.mp3 (3.1 MB)\n",
            "    üìÑ mark_08.mp3 (2.9 MB)\n",
            "    üìÑ mark_09.mp3 (3.9 MB)\n",
            "    üìÑ mark_10.mp3 (4.4 MB)\n",
            "    üìÑ mark_11.mp3 (2.7 MB)\n",
            "    üìÑ mark_12.mp3 (4.0 MB)\n",
            "    üìÑ mark_13.mp3 (2.7 MB)\n",
            "    üìÑ mark_14.mp3 (5.7 MB)\n",
            "    üìÑ mark_15.mp3 (3.6 MB)\n",
            "    üìÑ mark_16.mp3 (1.7 MB)\n",
            "  üìÅ Text/\n",
            "    üìÑ mark_01.txt (5.5 KB)\n",
            "    üìÑ mark_02.txt (4.2 KB)\n",
            "    üìÑ mark_03.txt (4.1 KB)\n",
            "    üìÑ mark_04.txt (5.3 KB)\n",
            "    üìÑ mark_05.txt (5.1 KB)\n",
            "    üìÑ mark_06.txt (943.0 B)\n",
            "    üìÑ mark_07.txt (5.2 KB)\n",
            "    üìÑ mark_08.txt (4.8 KB)\n",
            "    üìÑ mark_09.txt (6.4 KB)\n",
            "    üìÑ mark_10.txt (7.0 KB)\n",
            "    üìÑ mark_11.txt (4.4 KB)\n",
            "    üìÑ mark_12.txt (6.6 KB)\n",
            "    üìÑ mark_13.txt (4.8 KB)\n",
            "    üìÑ mark_14.txt (9.2 KB)\n",
            "    üìÑ mark_15.txt (5.3 KB)\n",
            "    üìÑ mark_16.txt (2.3 KB)\n",
            "  üìÅ whisper-hakha-chin/\n",
            "    üìÅ runs/\n",
            "      üìÅ Oct31_01-30-21_258f2580b535/\n",
            "        üìÑ events.out.tfevents.1761874223.258f2580b535.13787.0 (5.7 KB)\n",
            "\n",
            "============================================================\n",
            "TOTAL FILES: 45\n",
            "============================================================\n",
            "\n",
            "üîç CHECKING KEY FILES:\n",
            "------------------------------------------------------------\n",
            "‚úÖ aligned_training_data.json (160.5 KB)\n",
            "   ‚îî‚îÄ Contains 676 items\n",
            "   ‚îî‚îÄ Sample keys: ['audio', 'start', 'end', 'text', 'chapter']\n",
            "‚úÖ aligned_train_data.json (130.9 KB)\n",
            "   ‚îî‚îÄ Contains 540 items\n",
            "   ‚îî‚îÄ Sample keys: ['audio', 'start', 'end', 'text', 'chapter']\n",
            "‚úÖ aligned_val_data.json (29.7 KB)\n",
            "   ‚îî‚îÄ Contains 136 items\n",
            "   ‚îî‚îÄ Sample keys: ['audio', 'start', 'end', 'text', 'chapter']\n",
            "‚úÖ train_data.json (60.1 KB)\n",
            "   ‚îî‚îÄ Contains 12 items\n",
            "   ‚îî‚îÄ Sample keys: ['audio', 'text', 'chapter']\n",
            "‚úÖ val_data.json (22.9 KB)\n",
            "   ‚îî‚îÄ Contains 4 items\n",
            "   ‚îî‚îÄ Sample keys: ['audio', 'text', 'chapter']\n",
            "\n",
            "============================================================\n",
            "üìã COPY ALL OF THE ABOVE OUTPUT AND SEND IT BACK\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Run this in your Colab notebook to list all project files\n",
        "Copy the output and paste it back to me\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COLAB PROJECT FILES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get current directory\n",
        "current_dir = os.getcwd()\n",
        "print(f\"\\nCurrent Directory: {current_dir}\\n\")\n",
        "\n",
        "# Function to get file size in human-readable format\n",
        "def get_size(size_bytes):\n",
        "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
        "        if size_bytes < 1024.0:\n",
        "            return f\"{size_bytes:.1f} {unit}\"\n",
        "        size_bytes /= 1024.0\n",
        "    return f\"{size_bytes:.1f} TB\"\n",
        "\n",
        "# List all files recursively\n",
        "print(\"FILE STRUCTURE:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "all_files = []\n",
        "for root, dirs, files in os.walk(current_dir):\n",
        "    # Skip hidden directories and __pycache__\n",
        "    dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n",
        "\n",
        "    level = root.replace(current_dir, '').count(os.sep)\n",
        "    indent = '  ' * level\n",
        "    folder_name = os.path.basename(root) or 'ROOT'\n",
        "    print(f'{indent}üìÅ {folder_name}/')\n",
        "\n",
        "    subindent = '  ' * (level + 1)\n",
        "    for file in sorted(files):\n",
        "        if not file.startswith('.'):\n",
        "            filepath = os.path.join(root, file)\n",
        "            size = os.path.getsize(filepath)\n",
        "            print(f'{subindent}üìÑ {file} ({get_size(size)})')\n",
        "            all_files.append({\n",
        "                'path': filepath.replace(current_dir + '/', ''),\n",
        "                'size': size,\n",
        "                'size_readable': get_size(size)\n",
        "            })\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"TOTAL FILES: {len(all_files)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check for key training files\n",
        "print(\"\\nüîç CHECKING KEY FILES:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "key_files = [\n",
        "    'aligned_training_data.json',\n",
        "    'aligned_train_data.json',\n",
        "    'aligned_val_data.json',\n",
        "    'train_data.json',\n",
        "    'val_data.json'\n",
        "]\n",
        "\n",
        "for key_file in key_files:\n",
        "    if os.path.exists(key_file):\n",
        "        size = os.path.getsize(key_file)\n",
        "        print(f\"‚úÖ {key_file} ({get_size(size)})\")\n",
        "\n",
        "        # If it's a JSON file, show how many items\n",
        "        try:\n",
        "            with open(key_file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, list):\n",
        "                    print(f\"   ‚îî‚îÄ Contains {len(data)} items\")\n",
        "                    if len(data) > 0:\n",
        "                        print(f\"   ‚îî‚îÄ Sample keys: {list(data[0].keys())}\")\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        print(f\"‚ùå {key_file} (NOT FOUND)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã COPY ALL OF THE ABOVE OUTPUT AND SEND IT BACK\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzMeVeKnFNIr",
        "outputId": "b2278ce8-3d89-497e-a410-5171387dd6ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-31 03:39:24.103605: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761881964.123305   49075 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761881964.129296   49075 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761881964.144440   49075 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761881964.144467   49075 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761881964.144471   49075 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761881964.144476   49075 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "==================================================\n",
            "Whisper Fine-tuning for Hakha Chin\n",
            "FIXED VERSION - No Gradient Checkpointing Bug\n",
            "==================================================\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "üìÇ Loading aligned data...\n",
            "Train segments: 540\n",
            "Validation segments: 136\n",
            "\n",
            "üìã Sample training item:\n",
            "  Audio file: Audio/mark_05.mp3\n",
            "  Start time: 81.00s\n",
            "  End time: 83.00s\n",
            "  Text: Zaangfahnak tein ra law a cungah na kut ra chia ko sawh, cu ti na tuah ahcun a dam lai i a nung ko l...\n",
            "\n",
            "üîÑ Creating dataset objects...\n",
            "‚úÖ Dataset metadata ready\n",
            "\n",
            "ü§ñ Loading Whisper model...\n",
            "‚úÖ Model loaded (gradient checkpointing disabled)\n",
            "\n",
            "üîÑ Processing datasets...\n",
            "Processing training data:   0% 1/540 [00:01<14:19,  1.59s/ examples][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:   4% 20/540 [00:02<00:35, 14.79 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "Processing training data:   5% 28/540 [00:02<00:25, 19.75 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:   6% 31/540 [00:02<00:24, 20.77 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:   7% 37/540 [00:03<00:23, 21.15 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:   8% 41/540 [00:03<00:20, 23.94 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1984) too large for available bit count (1488)\n",
            "Processing training data:   8% 45/540 [00:03<00:20, 23.58 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2048) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:   9% 49/540 [00:03<00:18, 26.40 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  10% 52/540 [00:03<00:18, 25.85 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  11% 60/540 [00:03<00:17, 27.53 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  12% 63/540 [00:04<00:20, 23.01 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  12% 67/540 [00:04<00:19, 24.77 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  14% 77/540 [00:04<00:19, 24.09 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  15% 82/540 [00:04<00:16, 28.33 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  16% 87/540 [00:04<00:14, 31.24 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  17% 92/540 [00:05<00:13, 32.21 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1984) too large for available bit count (1488)\n",
            "Processing training data:  18% 98/540 [00:05<00:13, 31.73 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  19% 102/540 [00:05<00:13, 32.96 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  20% 107/540 [00:05<00:12, 34.77 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  22% 117/540 [00:05<00:12, 34.94 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2240) too large for available bit count (1488)\n",
            "Processing training data:  24% 129/540 [00:06<00:13, 30.11 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "Processing training data:  25% 135/540 [00:06<00:13, 29.93 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1488)\n",
            "Processing training data:  27% 144/540 [00:06<00:11, 33.29 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  28% 153/540 [00:06<00:11, 34.44 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  29% 157/540 [00:06<00:11, 34.05 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  30% 161/540 [00:07<00:11, 33.68 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2176) too large for available bit count (1488)\n",
            "Processing training data:  32% 175/540 [00:07<00:10, 34.70 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing training data:  36% 192/540 [00:08<00:09, 35.25 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  37% 198/540 [00:08<00:09, 36.01 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  38% 203/540 [00:08<00:09, 36.09 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing training data:  39% 213/540 [00:08<00:09, 34.97 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  40% 217/540 [00:08<00:09, 33.62 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  42% 227/540 [00:09<00:09, 34.11 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2304) too large for available bit count (1488)\n",
            "Processing training data:  44% 236/540 [00:09<00:09, 32.69 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  46% 249/540 [00:09<00:08, 35.04 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (1488)\n",
            "Processing training data:  49% 263/540 [00:10<00:07, 35.96 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  49% 267/540 [00:10<00:07, 34.44 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  50% 272/540 [00:10<00:07, 35.98 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  52% 281/540 [00:10<00:07, 33.98 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1488)\n",
            "Processing training data:  54% 290/540 [00:10<00:07, 34.52 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  54% 294/540 [00:10<00:07, 34.44 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "Processing training data:  61% 332/540 [00:12<00:05, 35.53 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  62% 336/540 [00:12<00:06, 33.90 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  67% 363/540 [00:12<00:04, 35.72 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  69% 372/540 [00:13<00:04, 33.89 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  70% 377/540 [00:13<00:04, 35.52 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  71% 381/540 [00:13<00:04, 35.50 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  72% 387/540 [00:13<00:03, 38.31 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  73% 392/540 [00:13<00:03, 38.13 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1488)\n",
            "Processing training data:  74% 401/540 [00:14<00:03, 35.44 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  75% 405/540 [00:14<00:03, 35.02 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2240) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing training data:  76% 409/540 [00:14<00:03, 34.58 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  78% 419/540 [00:14<00:03, 32.90 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "Processing training data:  78% 423/540 [00:14<00:04, 28.58 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  79% 426/540 [00:14<00:04, 26.44 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  81% 436/540 [00:15<00:04, 24.32 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2048) too large for available bit count (1488)\n",
            "Processing training data:  81% 440/540 [00:15<00:04, 23.83 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  83% 447/540 [00:15<00:03, 23.47 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  83% 450/540 [00:15<00:03, 23.02 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  84% 453/540 [00:16<00:04, 21.36 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1488)\n",
            "Processing training data:  86% 467/540 [00:16<00:03, 22.31 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1984) too large for available bit count (1488)\n",
            "Processing training data:  88% 474/540 [00:16<00:02, 24.53 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  90% 484/540 [00:17<00:02, 24.42 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1984) too large for available bit count (1488)\n",
            "Processing training data:  92% 495/540 [00:17<00:01, 24.89 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2240) too large for available bit count (1488)\n",
            "Processing training data:  92% 499/540 [00:18<00:01, 24.55 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  94% 509/540 [00:18<00:01, 30.74 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing training data:  97% 523/540 [00:18<00:00, 34.09 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  98% 528/540 [00:18<00:00, 34.50 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data: 100% 538/540 [00:19<00:00, 33.79 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "Processing training data: 100% 540/540 [00:20<00:00, 26.05 examples/s]\n",
            "Processing validation data:   0% 0/136 [00:00<?, ? examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing validation data:   4% 6/136 [00:00<00:03, 40.12 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing validation data:  12% 17/136 [00:00<00:03, 35.57 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing validation data:  26% 35/136 [00:00<00:02, 34.17 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing validation data:  33% 45/136 [00:01<00:02, 36.18 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2432) too large for available bit count (1488)\n",
            "Processing validation data:  36% 49/136 [00:01<00:02, 34.92 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "Processing validation data:  42% 57/136 [00:01<00:02, 30.40 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing validation data:  69% 94/136 [00:02<00:01, 34.96 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2240) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing validation data:  73% 99/136 [00:02<00:01, 36.36 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "Processing validation data:  76% 103/136 [00:02<00:00, 35.72 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing validation data:  79% 107/136 [00:03<00:00, 34.26 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2048) too large for available bit count (1488)\n",
            "Processing validation data:  95% 129/136 [00:03<00:00, 31.69 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing validation data: 100% 136/136 [00:04<00:00, 31.87 examples/s]\n",
            "‚úÖ Data pipeline ready\n",
            "\n",
            "üöÄ Starting training...\n",
            "==================================================\n",
            "üìä Training on 540 segments\n",
            "üìä Validating on 136 segments\n",
            "üìä Batch size: 4, Accumulation: 4 (Effective: 16)\n",
            "üìä Total steps: ~165\n",
            "==================================================\n",
            "  0% 0/170 [00:00<?, ?it/s]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 15% 25/170 [01:15<07:23,  3.06s/it]Step 25: {'loss': 6.4615, 'grad_norm': 19.79622459411621, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.7407407407407407}\n",
            "{'loss': 6.4615, 'grad_norm': 19.79622459411621, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.74}\n",
            " 20% 34/170 [01:39<05:36,  2.48s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 29% 50/170 [02:26<05:11,  2.59s/it]Step 50: {'loss': 5.1112, 'grad_norm': 13.900821685791016, 'learning_rate': 4.9000000000000005e-06, 'epoch': 1.474074074074074}\n",
            "{'loss': 5.1112, 'grad_norm': 13.900821685791016, 'learning_rate': 4.9000000000000005e-06, 'epoch': 1.47}\n",
            " 40% 68/170 [03:16<04:13,  2.49s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 44% 75/170 [03:39<05:07,  3.23s/it]Step 75: {'loss': 4.1179, 'grad_norm': 12.171441078186035, 'learning_rate': 7.4e-06, 'epoch': 2.2074074074074073}\n",
            "{'loss': 4.1179, 'grad_norm': 12.171441078186035, 'learning_rate': 7.4e-06, 'epoch': 2.21}\n",
            " 59% 100/170 [04:49<03:41,  3.17s/it]Step 100: {'loss': 3.3892, 'grad_norm': 12.45138931274414, 'learning_rate': 9.9e-06, 'epoch': 2.948148148148148}\n",
            "{'loss': 3.3892, 'grad_norm': 12.45138931274414, 'learning_rate': 9.9e-06, 'epoch': 2.95}\n",
            " 60% 102/170 [04:53<02:44,  2.42s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 74% 125/170 [06:01<02:21,  3.14s/it]Step 125: {'loss': 2.9337, 'grad_norm': 12.278806686401367, 'learning_rate': 6.571428571428572e-06, 'epoch': 3.6814814814814816}\n",
            "{'loss': 2.9337, 'grad_norm': 12.278806686401367, 'learning_rate': 6.571428571428572e-06, 'epoch': 3.68}\n",
            " 80% 136/170 [06:31<01:29,  2.64s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 88% 150/170 [07:12<00:54,  2.73s/it]Step 150: {'loss': 2.6786, 'grad_norm': 11.835489273071289, 'learning_rate': 3e-06, 'epoch': 4.4148148148148145}\n",
            "{'loss': 2.6786, 'grad_norm': 11.835489273071289, 'learning_rate': 3e-06, 'epoch': 4.41}\n",
            "100% 170/170 [08:08<00:00,  2.66s/it]/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "Step 170: {'train_runtime': 517.482, 'train_samples_per_second': 5.218, 'train_steps_per_second': 0.329, 'total_flos': 7.79180580864e+17, 'train_loss': 3.9378010918112363, 'epoch': 5.0}\n",
            "{'train_runtime': 517.482, 'train_samples_per_second': 5.218, 'train_steps_per_second': 0.329, 'train_loss': 3.9378010918112363, 'epoch': 5.0}\n",
            "100% 170/170 [08:37<00:00,  3.04s/it]\n",
            "\n",
            "‚úÖ Training complete!\n",
            "üìÅ Model saved to: ./whisper-hakha-chin\n",
            "\n",
            "üéâ All done! Your fine-tuned Hakha Chin Whisper model is ready!\n"
          ]
        }
      ],
      "source": [
        "!python fine-tuning-aligned.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we1UCNdcFduW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5L8Ht4khDgp",
        "outputId": "de2f7cd9-cc26-4532-ce12-eee4288d3e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-31 04:26:35.868005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761884795.889034   61065 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761884795.895356   61065 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761884795.914161   61065 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761884795.914189   61065 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761884795.914193   61065 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761884795.914198   61065 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "==================================================\n",
            "CONTINUING TRAINING - Hakha Chin Whisper\n",
            "Version 2: More Epochs + Anti-Repetition\n",
            "==================================================\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "üìÇ Loading aligned data...\n",
            "Train segments: 540\n",
            "Validation segments: 136\n",
            "\n",
            "üîÑ Creating dataset objects...\n",
            "\n",
            "ü§ñ Loading model from checkpoint: ./whisper-hakha-chin\n",
            "‚úÖ Loaded from checkpoint - continuing training!\n",
            "‚úÖ Model ready (with anti-repetition settings)\n",
            "\n",
            "üîÑ Processing datasets...\n",
            "Processing training data:   0% 1/540 [00:01<15:56,  1.78s/ examples][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:   4% 21/540 [00:02<00:31, 16.38 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "Processing training data:   5% 27/540 [00:02<00:24, 21.31 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:   7% 37/540 [00:02<00:20, 25.09 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1984) too large for available bit count (1488)\n",
            "Processing training data:   9% 46/540 [00:03<00:17, 28.02 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2048) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:   9% 51/540 [00:03<00:16, 29.42 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  11% 61/540 [00:03<00:15, 30.93 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  12% 65/540 [00:03<00:16, 29.43 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  13% 69/540 [00:03<00:15, 30.31 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  14% 77/540 [00:04<00:15, 29.72 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  16% 85/540 [00:04<00:15, 29.18 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  16% 89/540 [00:04<00:15, 29.52 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  18% 97/540 [00:04<00:15, 28.03 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1984) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  19% 101/540 [00:05<00:15, 28.39 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  20% 106/540 [00:05<00:13, 31.43 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  22% 118/540 [00:05<00:13, 30.45 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2240) too large for available bit count (1488)\n",
            "Processing training data:  24% 131/540 [00:06<00:14, 28.36 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "Processing training data:  25% 135/540 [00:06<00:14, 28.53 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1488)\n",
            "Processing training data:  27% 147/540 [00:06<00:12, 30.55 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  29% 155/540 [00:06<00:12, 30.05 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  29% 159/540 [00:07<00:12, 30.66 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2176) too large for available bit count (1488)\n",
            "Processing training data:  32% 172/540 [00:07<00:11, 33.26 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing training data:  35% 189/540 [00:08<00:12, 28.22 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  36% 197/540 [00:08<00:11, 30.70 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  37% 201/540 [00:08<00:10, 31.66 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing training data:  40% 214/540 [00:08<00:10, 32.36 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  41% 219/540 [00:09<00:11, 28.08 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  42% 228/540 [00:09<00:10, 31.09 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2304) too large for available bit count (1488)\n",
            "Processing training data:  44% 236/540 [00:09<00:09, 31.06 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  46% 248/540 [00:09<00:09, 32.22 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  47% 253/540 [00:10<00:08, 31.99 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (1488)\n",
            "Processing training data:  49% 262/540 [00:10<00:08, 31.91 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  49% 266/540 [00:10<00:09, 30.28 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  50% 271/540 [00:10<00:08, 32.77 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  52% 283/540 [00:11<00:08, 28.87 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1488)\n",
            "Processing training data:  53% 287/540 [00:11<00:08, 30.93 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  55% 295/540 [00:11<00:08, 29.47 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "Processing training data:  61% 332/540 [00:12<00:08, 24.16 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  63% 339/540 [00:13<00:09, 20.12 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  67% 364/540 [00:14<00:09, 19.31 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  69% 374/540 [00:15<00:08, 18.68 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  70% 380/540 [00:15<00:08, 18.77 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  71% 384/540 [00:15<00:07, 21.24 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  72% 388/540 [00:15<00:06, 22.87 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  73% 392/540 [00:15<00:05, 25.63 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1488)\n",
            "Processing training data:  74% 401/540 [00:16<00:04, 28.32 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  75% 405/540 [00:16<00:04, 29.54 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2240) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing training data:  76% 409/540 [00:16<00:04, 29.73 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  78% 421/540 [00:16<00:04, 29.57 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  79% 425/540 [00:16<00:03, 29.24 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  80% 433/540 [00:17<00:03, 28.15 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  81% 437/540 [00:17<00:03, 29.34 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2048) too large for available bit count (1488)\n",
            "Processing training data:  82% 441/540 [00:17<00:03, 28.21 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing training data:  82% 445/540 [00:17<00:03, 28.74 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  83% 449/540 [00:17<00:03, 28.35 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  84% 453/540 [00:17<00:03, 27.67 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1488)\n",
            "Processing training data:  86% 467/540 [00:18<00:02, 29.03 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1984) too large for available bit count (1488)\n",
            "Processing training data:  87% 471/540 [00:18<00:02, 28.80 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing training data:  90% 484/540 [00:18<00:01, 29.94 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1984) too large for available bit count (1488)\n",
            "Processing training data:  92% 496/540 [00:19<00:01, 30.80 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2240) too large for available bit count (1488)\n",
            "Processing training data:  93% 500/540 [00:19<00:01, 30.91 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data:  94% 509/540 [00:19<00:01, 30.77 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing training data:  97% 526/540 [00:20<00:00, 33.63 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "Processing training data:  98% 530/540 [00:20<00:00, 32.28 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing training data: 100% 538/540 [00:20<00:00, 28.73 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "Processing training data: 100% 540/540 [00:22<00:00, 24.04 examples/s]\n",
            "Processing validation data:   0% 0/136 [00:00<?, ? examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing validation data:   4% 6/136 [00:00<00:03, 37.02 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing validation data:  10% 14/136 [00:00<00:03, 31.23 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing validation data:  26% 36/136 [00:01<00:03, 32.38 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing validation data:  34% 46/136 [00:01<00:03, 28.33 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2432) too large for available bit count (1488)\n",
            "Processing validation data:  37% 50/136 [00:01<00:03, 28.61 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "Processing validation data:  43% 58/136 [00:01<00:02, 28.44 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing validation data:  70% 95/136 [00:03<00:01, 24.29 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2240) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1856) too large for available bit count (1488)\n",
            "Processing validation data:  73% 99/136 [00:03<00:01, 26.57 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "Processing validation data:  76% 103/136 [00:03<00:01, 25.39 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1488)\n",
            "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1600) too large for available bit count (1488)\n",
            "Processing validation data:  79% 107/136 [00:03<00:01, 23.84 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1728) too large for available bit count (1488)\n",
            "Processing validation data:  81% 110/136 [00:03<00:01, 24.01 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2048) too large for available bit count (1488)\n",
            "Processing validation data:  96% 130/136 [00:04<00:00, 21.63 examples/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1536) too large for available bit count (1488)\n",
            "Processing validation data: 100% 136/136 [00:05<00:00, 25.12 examples/s]\n",
            "‚úÖ Data ready\n",
            "\n",
            "üöÄ RESUMING TRAINING...\n",
            "==================================================\n",
            "üìä Training on 540 segments\n",
            "üìä Validating on 136 segments\n",
            "üìä Epochs: 15 (was 5)\n",
            "üìä Steps per epoch: ~33\n",
            "üìä Total steps: ~495\n",
            "üìä Anti-repetition: ENABLED\n",
            "==================================================\n",
            "  0% 0/510 [00:00<?, ?it/s]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "  4% 20/510 [00:57<20:52,  2.56s/it]Step 20: {'loss': 2.5114, 'grad_norm': 11.40009880065918, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.5882352941176471}\n",
            "{'loss': 2.5114, 'grad_norm': 11.40009880065918, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.59}\n",
            "  7% 34/510 [01:30<16:04,  2.03s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "  8% 40/510 [01:51<22:43,  2.90s/it]Step 40: {'loss': 2.4637, 'grad_norm': 11.27294921875, 'learning_rate': 3.900000000000001e-06, 'epoch': 1.1764705882352942}\n",
            "{'loss': 2.4637, 'grad_norm': 11.27294921875, 'learning_rate': 3.900000000000001e-06, 'epoch': 1.18}\n",
            " 12% 60/510 [02:42<18:44,  2.50s/it]Step 60: {'loss': 2.4191, 'grad_norm': 11.956681251525879, 'learning_rate': 4.902173913043479e-06, 'epoch': 1.7647058823529411}\n",
            "{'loss': 2.4191, 'grad_norm': 11.956681251525879, 'learning_rate': 4.902173913043479e-06, 'epoch': 1.76}\n",
            " 13% 68/510 [03:01<16:11,  2.20s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 16% 80/510 [03:36<17:33,  2.45s/it]Step 80: {'loss': 2.3052, 'grad_norm': 13.274781227111816, 'learning_rate': 4.684782608695653e-06, 'epoch': 2.3529411764705883}\n",
            "{'loss': 2.3052, 'grad_norm': 13.274781227111816, 'learning_rate': 4.684782608695653e-06, 'epoch': 2.35}\n",
            " 20% 100/510 [04:27<17:02,  2.49s/it]Step 100: {'loss': 2.198, 'grad_norm': 12.119531631469727, 'learning_rate': 4.467391304347826e-06, 'epoch': 2.9411764705882355}\n",
            "{'loss': 2.198, 'grad_norm': 12.119531631469727, 'learning_rate': 4.467391304347826e-06, 'epoch': 2.94}\n",
            " 20% 100/510 [04:27<17:02,  2.49s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
            "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
            "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n",
            "\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/17 [00:02<00:17,  1.19s/it]\u001b[A\n",
            " 18% 3/17 [00:04<00:23,  1.68s/it]\u001b[A\n",
            " 24% 4/17 [00:07<00:29,  2.25s/it]\u001b[A\n",
            " 29% 5/17 [00:11<00:31,  2.62s/it]\u001b[A\n",
            " 35% 6/17 [00:14<00:29,  2.66s/it]\u001b[A\n",
            " 41% 7/17 [00:16<00:25,  2.58s/it]\u001b[A\n",
            " 47% 8/17 [00:18<00:22,  2.55s/it]\u001b[A\n",
            " 53% 9/17 [00:21<00:19,  2.47s/it]\u001b[A\n",
            " 59% 10/17 [00:24<00:18,  2.67s/it]\u001b[A\n",
            " 65% 11/17 [00:27<00:16,  2.70s/it]\u001b[A\n",
            " 71% 12/17 [00:29<00:13,  2.62s/it]\u001b[A\n",
            " 76% 13/17 [00:31<00:10,  2.54s/it]\u001b[A\n",
            " 82% 14/17 [00:34<00:07,  2.55s/it]\u001b[A\n",
            " 88% 15/17 [00:36<00:05,  2.52s/it]\u001b[A\n",
            " 94% 16/17 [00:39<00:02,  2.57s/it]\u001b[A\n",
            "100% 17/17 [00:41<00:00,  2.52s/it]\u001b[AStep 100: {'eval_loss': 2.9391214847564697, 'eval_wer': 99.36793080505656, 'eval_runtime': 61.619, 'eval_samples_per_second': 2.207, 'eval_steps_per_second': 0.276, 'epoch': 2.9411764705882355}\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.9391214847564697, 'eval_wer': 99.36793080505656, 'eval_runtime': 61.619, 'eval_samples_per_second': 2.207, 'eval_steps_per_second': 0.276, 'epoch': 2.94}\n",
            " 20% 100/510 [05:29<17:02,  2.49s/it]\n",
            "100% 17/17 [00:52<00:00,  2.52s/it]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'suppress_tokens': []}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            " 20% 102/510 [06:15<2:43:18, 24.02s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 24% 120/510 [07:20<18:11,  2.80s/it]Step 120: {'loss': 2.0452, 'grad_norm': 12.83048152923584, 'learning_rate': 4.25e-06, 'epoch': 3.5294117647058822}\n",
            "{'loss': 2.0452, 'grad_norm': 12.83048152923584, 'learning_rate': 4.25e-06, 'epoch': 3.53}\n",
            " 27% 136/510 [07:59<12:57,  2.08s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 27% 140/510 [08:14<19:08,  3.11s/it]Step 140: {'loss': 2.0257, 'grad_norm': 13.084433555603027, 'learning_rate': 4.032608695652174e-06, 'epoch': 4.117647058823529}\n",
            "{'loss': 2.0257, 'grad_norm': 13.084433555603027, 'learning_rate': 4.032608695652174e-06, 'epoch': 4.12}\n",
            " 31% 160/510 [09:05<14:32,  2.49s/it]Step 160: {'loss': 1.8723, 'grad_norm': 13.977042198181152, 'learning_rate': 3.815217391304349e-06, 'epoch': 4.705882352941177}\n",
            "{'loss': 1.8723, 'grad_norm': 13.977042198181152, 'learning_rate': 3.815217391304349e-06, 'epoch': 4.71}\n",
            " 33% 170/510 [09:30<11:53,  2.10s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 35% 180/510 [10:00<13:39,  2.48s/it]Step 180: {'loss': 1.8234, 'grad_norm': 15.587709426879883, 'learning_rate': 3.5978260869565218e-06, 'epoch': 5.294117647058823}\n",
            "{'loss': 1.8234, 'grad_norm': 15.587709426879883, 'learning_rate': 3.5978260869565218e-06, 'epoch': 5.29}\n",
            " 39% 200/510 [10:52<13:05,  2.53s/it]Step 200: {'loss': 1.7293, 'grad_norm': 13.28185749053955, 'learning_rate': 3.3804347826086957e-06, 'epoch': 5.882352941176471}\n",
            "{'loss': 1.7293, 'grad_norm': 13.28185749053955, 'learning_rate': 3.3804347826086957e-06, 'epoch': 5.88}\n",
            " 39% 200/510 [10:52<13:05,  2.53s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/17 [00:02<00:17,  1.16s/it]\u001b[A\n",
            " 18% 3/17 [00:04<00:23,  1.71s/it]\u001b[A\n",
            " 24% 4/17 [00:07<00:26,  2.01s/it]\u001b[A\n",
            " 29% 5/17 [00:10<00:27,  2.30s/it]\u001b[A\n",
            " 35% 6/17 [00:13<00:27,  2.53s/it]\u001b[A\n",
            " 41% 7/17 [00:15<00:26,  2.63s/it]\u001b[A\n",
            " 47% 8/17 [00:18<00:23,  2.61s/it]\u001b[A\n",
            " 53% 9/17 [00:21<00:21,  2.69s/it]\u001b[A\n",
            " 59% 10/17 [00:25<00:21,  3.04s/it]\u001b[A\n",
            " 65% 11/17 [00:28<00:17,  2.96s/it]\u001b[A\n",
            " 71% 12/17 [00:30<00:14,  2.84s/it]\u001b[A\n",
            " 76% 13/17 [00:33<00:10,  2.74s/it]\u001b[A\n",
            " 82% 14/17 [00:35<00:07,  2.66s/it]\u001b[A\n",
            " 88% 15/17 [00:38<00:05,  2.81s/it]\u001b[A\n",
            " 94% 16/17 [00:41<00:02,  2.73s/it]\u001b[A\n",
            "100% 17/17 [00:43<00:00,  2.71s/it]\u001b[AStep 200: {'eval_loss': 3.0284528732299805, 'eval_wer': 101.0645375914837, 'eval_runtime': 63.774, 'eval_samples_per_second': 2.133, 'eval_steps_per_second': 0.267, 'epoch': 5.882352941176471}\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.0284528732299805, 'eval_wer': 101.0645375914837, 'eval_runtime': 63.774, 'eval_samples_per_second': 2.133, 'eval_steps_per_second': 0.267, 'epoch': 5.88}\n",
            " 39% 200/510 [11:56<13:05,  2.53s/it]\n",
            "100% 17/17 [00:54<00:00,  2.71s/it]\u001b[A\n",
            " 40% 204/510 [12:36<1:01:50, 12.12s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 43% 220/510 [13:33<13:33,  2.80s/it]Step 220: {'loss': 1.6101, 'grad_norm': 14.181112289428711, 'learning_rate': 3.16304347826087e-06, 'epoch': 6.470588235294118}\n",
            "{'loss': 1.6101, 'grad_norm': 14.181112289428711, 'learning_rate': 3.16304347826087e-06, 'epoch': 6.47}\n",
            " 47% 238/510 [14:18<09:43,  2.14s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 47% 240/510 [14:29<15:32,  3.45s/it]Step 240: {'loss': 1.5148, 'grad_norm': 12.679821014404297, 'learning_rate': 2.945652173913044e-06, 'epoch': 7.0588235294117645}\n",
            "{'loss': 1.5148, 'grad_norm': 12.679821014404297, 'learning_rate': 2.945652173913044e-06, 'epoch': 7.06}\n",
            " 51% 260/510 [15:21<11:24,  2.74s/it]Step 260: {'loss': 1.416, 'grad_norm': 14.026391983032227, 'learning_rate': 2.7282608695652175e-06, 'epoch': 7.647058823529412}\n",
            "{'loss': 1.416, 'grad_norm': 14.026391983032227, 'learning_rate': 2.7282608695652175e-06, 'epoch': 7.65}\n",
            " 53% 272/510 [15:49<08:13,  2.08s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 55% 280/510 [16:14<10:37,  2.77s/it]Step 280: {'loss': 1.3656, 'grad_norm': 15.739072799682617, 'learning_rate': 2.5108695652173914e-06, 'epoch': 8.235294117647058}\n",
            "{'loss': 1.3656, 'grad_norm': 15.739072799682617, 'learning_rate': 2.5108695652173914e-06, 'epoch': 8.24}\n",
            " 59% 300/510 [17:05<08:31,  2.44s/it]Step 300: {'loss': 1.2816, 'grad_norm': 15.205245971679688, 'learning_rate': 2.2934782608695654e-06, 'epoch': 8.823529411764707}\n",
            "{'loss': 1.2816, 'grad_norm': 15.205245971679688, 'learning_rate': 2.2934782608695654e-06, 'epoch': 8.82}\n",
            " 59% 300/510 [17:05<08:31,  2.44s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/17 [00:02<00:19,  1.30s/it]\u001b[A\n",
            " 18% 3/17 [00:05<00:24,  1.76s/it]\u001b[A\n",
            " 24% 4/17 [00:08<00:32,  2.48s/it]\u001b[A\n",
            " 29% 5/17 [00:11<00:30,  2.57s/it]\u001b[A\n",
            " 35% 6/17 [00:13<00:27,  2.50s/it]\u001b[A\n",
            " 41% 7/17 [00:16<00:24,  2.47s/it]\u001b[A\n",
            " 47% 8/17 [00:18<00:22,  2.54s/it]\u001b[A\n",
            " 53% 9/17 [00:22<00:22,  2.76s/it]\u001b[A\n",
            " 59% 10/17 [00:25<00:19,  2.80s/it]\u001b[A\n",
            " 65% 11/17 [00:27<00:16,  2.76s/it]\u001b[A\n",
            " 71% 12/17 [00:30<00:14,  2.88s/it]\u001b[A\n",
            " 76% 13/17 [00:33<00:10,  2.73s/it]\u001b[A\n",
            " 82% 14/17 [00:36<00:08,  2.85s/it]\u001b[A\n",
            " 88% 15/17 [00:38<00:05,  2.71s/it]\u001b[A\n",
            " 94% 16/17 [00:41<00:02,  2.67s/it]\u001b[A\n",
            "100% 17/17 [00:44<00:00,  2.68s/it]\u001b[AStep 300: {'eval_loss': 3.2031662464141846, 'eval_wer': 101.66333998669327, 'eval_runtime': 63.4651, 'eval_samples_per_second': 2.143, 'eval_steps_per_second': 0.268, 'epoch': 8.823529411764707}\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.2031662464141846, 'eval_wer': 101.66333998669327, 'eval_runtime': 63.4651, 'eval_samples_per_second': 2.143, 'eval_steps_per_second': 0.268, 'epoch': 8.82}\n",
            " 59% 300/510 [18:09<08:31,  2.44s/it]\n",
            "100% 17/17 [00:54<00:00,  2.68s/it]\u001b[A\n",
            " 60% 306/510 [19:09<27:17,  8.03s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 63% 320/510 [19:57<08:11,  2.59s/it]Step 320: {'loss': 1.2371, 'grad_norm': 13.590353965759277, 'learning_rate': 2.0760869565217393e-06, 'epoch': 9.411764705882353}\n",
            "{'loss': 1.2371, 'grad_norm': 13.590353965759277, 'learning_rate': 2.0760869565217393e-06, 'epoch': 9.41}\n",
            " 67% 340/510 [20:49<06:12,  2.19s/it]Step 340: {'loss': 1.1597, 'grad_norm': 23.046611785888672, 'learning_rate': 1.8586956521739132e-06, 'epoch': 10.0}\n",
            "{'loss': 1.1597, 'grad_norm': 23.046611785888672, 'learning_rate': 1.8586956521739132e-06, 'epoch': 10.0}\n",
            " 67% 340/510 [20:49<06:12,  2.19s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 71% 360/510 [21:45<06:28,  2.59s/it]Step 360: {'loss': 1.081, 'grad_norm': 13.795768737792969, 'learning_rate': 1.6413043478260872e-06, 'epoch': 10.588235294117647}\n",
            "{'loss': 1.081, 'grad_norm': 13.795768737792969, 'learning_rate': 1.6413043478260872e-06, 'epoch': 10.59}\n",
            " 73% 374/510 [22:19<04:43,  2.08s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 75% 380/510 [22:39<05:57,  2.75s/it]Step 380: {'loss': 1.0174, 'grad_norm': 12.840668678283691, 'learning_rate': 1.423913043478261e-06, 'epoch': 11.176470588235293}\n",
            "{'loss': 1.0174, 'grad_norm': 12.840668678283691, 'learning_rate': 1.423913043478261e-06, 'epoch': 11.18}\n",
            " 78% 400/510 [23:31<05:00,  2.73s/it]Step 400: {'loss': 0.9924, 'grad_norm': 13.605695724487305, 'learning_rate': 1.2065217391304348e-06, 'epoch': 11.764705882352942}\n",
            "{'loss': 0.9924, 'grad_norm': 13.605695724487305, 'learning_rate': 1.2065217391304348e-06, 'epoch': 11.76}\n",
            " 78% 400/510 [23:31<05:00,  2.73s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/17 [00:02<00:21,  1.41s/it]\u001b[A\n",
            " 18% 3/17 [00:06<00:30,  2.18s/it]\u001b[A\n",
            " 24% 4/17 [00:08<00:30,  2.37s/it]\u001b[A\n",
            " 29% 5/17 [00:11<00:29,  2.44s/it]\u001b[A\n",
            " 35% 6/17 [00:14<00:29,  2.71s/it]\u001b[A\n",
            " 41% 7/17 [00:18<00:32,  3.24s/it]\u001b[A\n",
            " 47% 8/17 [00:21<00:27,  3.07s/it]\u001b[A\n",
            " 53% 9/17 [00:24<00:23,  2.99s/it]\u001b[A\n",
            " 59% 10/17 [00:27<00:20,  2.99s/it]\u001b[A\n",
            " 65% 11/17 [00:30<00:18,  3.13s/it]\u001b[A\n",
            " 71% 12/17 [00:33<00:14,  2.95s/it]\u001b[A\n",
            " 76% 13/17 [00:36<00:11,  2.98s/it]\u001b[A\n",
            " 82% 14/17 [00:38<00:08,  2.75s/it]\u001b[A\n",
            " 88% 15/17 [00:41<00:05,  2.68s/it]\u001b[A\n",
            " 94% 16/17 [00:44<00:02,  2.92s/it]\u001b[A\n",
            "100% 17/17 [00:47<00:00,  2.97s/it]\u001b[AStep 400: {'eval_loss': 3.345796823501587, 'eval_wer': 103.62608117099134, 'eval_runtime': 65.7733, 'eval_samples_per_second': 2.068, 'eval_steps_per_second': 0.258, 'epoch': 11.764705882352942}\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.345796823501587, 'eval_wer': 103.62608117099134, 'eval_runtime': 65.7733, 'eval_samples_per_second': 2.068, 'eval_steps_per_second': 0.258, 'epoch': 11.76}\n",
            " 78% 400/510 [24:37<05:00,  2.73s/it]\n",
            "100% 17/17 [00:58<00:00,  2.97s/it]\u001b[A\n",
            " 80% 408/510 [25:36<08:39,  5.09s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 82% 420/510 [26:17<03:58,  2.65s/it]Step 420: {'loss': 0.9272, 'grad_norm': 13.658744812011719, 'learning_rate': 9.891304347826087e-07, 'epoch': 12.352941176470589}\n",
            "{'loss': 0.9272, 'grad_norm': 13.658744812011719, 'learning_rate': 9.891304347826087e-07, 'epoch': 12.35}\n",
            " 86% 440/510 [27:09<03:10,  2.72s/it]Step 440: {'loss': 0.936, 'grad_norm': 12.467523574829102, 'learning_rate': 7.717391304347827e-07, 'epoch': 12.941176470588236}\n",
            "{'loss': 0.936, 'grad_norm': 12.467523574829102, 'learning_rate': 7.717391304347827e-07, 'epoch': 12.94}\n",
            " 87% 442/510 [27:12<02:27,  2.18s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 90% 460/510 [28:05<02:13,  2.68s/it]Step 460: {'loss': 0.8762, 'grad_norm': 12.594215393066406, 'learning_rate': 5.543478260869566e-07, 'epoch': 13.529411764705882}\n",
            "{'loss': 0.8762, 'grad_norm': 12.594215393066406, 'learning_rate': 5.543478260869566e-07, 'epoch': 13.53}\n",
            " 93% 476/510 [28:43<01:08,  2.02s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 94% 480/510 [28:58<01:28,  2.96s/it]Step 480: {'loss': 0.8554, 'grad_norm': 12.512959480285645, 'learning_rate': 3.369565217391305e-07, 'epoch': 14.117647058823529}\n",
            "{'loss': 0.8554, 'grad_norm': 12.512959480285645, 'learning_rate': 3.369565217391305e-07, 'epoch': 14.12}\n",
            " 98% 500/510 [29:50<00:25,  2.56s/it]Step 500: {'loss': 0.8229, 'grad_norm': 11.6837158203125, 'learning_rate': 1.1956521739130436e-07, 'epoch': 14.705882352941176}\n",
            "{'loss': 0.8229, 'grad_norm': 11.6837158203125, 'learning_rate': 1.1956521739130436e-07, 'epoch': 14.71}\n",
            " 98% 500/510 [29:50<00:25,  2.56s/it]You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/17 [00:03<00:23,  1.59s/it]\u001b[A\n",
            " 18% 3/17 [00:06<00:33,  2.38s/it]\u001b[A\n",
            " 24% 4/17 [00:09<00:31,  2.39s/it]\u001b[A\n",
            " 29% 5/17 [00:11<00:29,  2.42s/it]\u001b[A\n",
            " 35% 6/17 [00:14<00:28,  2.57s/it]\u001b[A\n",
            " 41% 7/17 [00:17<00:25,  2.58s/it]\u001b[A\n",
            " 47% 8/17 [00:20<00:25,  2.85s/it]\u001b[A\n",
            " 53% 9/17 [00:22<00:21,  2.69s/it]\u001b[A\n",
            " 59% 10/17 [00:25<00:19,  2.79s/it]\u001b[A\n",
            " 65% 11/17 [00:28<00:16,  2.81s/it]\u001b[A\n",
            " 71% 12/17 [00:31<00:13,  2.77s/it]\u001b[A\n",
            " 76% 13/17 [00:33<00:10,  2.71s/it]\u001b[A\n",
            " 82% 14/17 [00:36<00:08,  2.78s/it]\u001b[A\n",
            " 88% 15/17 [00:39<00:05,  2.79s/it]\u001b[A\n",
            " 94% 16/17 [00:42<00:02,  2.72s/it]\u001b[A\n",
            "100% 17/17 [00:47<00:00,  3.53s/it]\u001b[AStep 500: {'eval_loss': 3.426975965499878, 'eval_wer': 101.66333998669327, 'eval_runtime': 64.9596, 'eval_samples_per_second': 2.094, 'eval_steps_per_second': 0.262, 'epoch': 14.705882352941176}\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.426975965499878, 'eval_wer': 101.66333998669327, 'eval_runtime': 64.9596, 'eval_samples_per_second': 2.094, 'eval_steps_per_second': 0.262, 'epoch': 14.71}\n",
            " 98% 500/510 [30:55<00:25,  2.56s/it]\n",
            "100% 17/17 [00:57<00:00,  3.53s/it]\u001b[A\n",
            "100% 510/510 [32:13<00:00,  3.88s/it]There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n",
            "Step 510: {'train_runtime': 1990.8199, 'train_samples_per_second': 4.069, 'train_steps_per_second': 0.256, 'total_flos': 2.337541742592e+18, 'train_loss': 1.5260803615345675, 'epoch': 15.0}\n",
            "{'train_runtime': 1990.8199, 'train_samples_per_second': 4.069, 'train_steps_per_second': 0.256, 'train_loss': 1.5260803615345675, 'epoch': 15.0}\n",
            "100% 510/510 [33:10<00:00,  3.90s/it]\n",
            "\n",
            "‚úÖ Training complete!\n",
            "üìÅ Model saved to: ./whisper-hakha-chin-v2\n",
            "\n",
            "üéâ Version 2 complete!\n",
            "Next: Test with the Gradio interface!\n"
          ]
        }
      ],
      "source": [
        "!python continue_training.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "# Load model\n",
        "processor = WhisperProcessor.from_pretrained(\"./whisper-hakha-chin-v2\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-hakha-chin-v2\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "print(\"Model config:\")\n",
        "print(f\"  forced_decoder_ids: {model.config.forced_decoder_ids}\")\n",
        "print(f\"  suppress_tokens: {model.config.suppress_tokens}\")\n",
        "\n",
        "# Test on ACTUAL training file\n",
        "audio, _ = librosa.load(\"Audio/mark_01.mp3\", sr=16000, duration=10)  # First 10 seconds\n",
        "input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "input_features = input_features.to(\"cuda\")\n",
        "\n",
        "# Generate\n",
        "predicted_ids = model.generate(\n",
        "    input_features,\n",
        "    max_length=200,\n",
        "    repetition_penalty=1.5,\n",
        "    no_repeat_ngram_size=3\n",
        ")\n",
        "result = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(f\"\\nTest result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLJ290qRQ4Gy",
        "outputId": "609eca35-a0ca-47c8-ea45-9192ef82590f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model config:\n",
            "  forced_decoder_ids: None\n",
            "  suppress_tokens: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
            "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
            "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test result: Mirimah ·π≠ha hlak chung ah khan an ra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/ChinTranslator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ9FbOWtTw_g",
        "outputId": "ff5a3ea7-688a-45a6-ee3c-b0e59bfc1e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/ChinTranslator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gradio_interface.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MhT-sOiVy_X",
        "outputId": "58f0567e-6711-4767-fa87-37b811df4f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-31 09:01:19.183504: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761901279.205407   20136 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761901279.211921   20136 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761901279.228225   20136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761901279.228254   20136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761901279.228257   20136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761901279.228260   20136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "ü§ñ Loading Hakha Chin Whisper V2...\n",
            "‚ö†Ô∏è Using auto-detect\n",
            "‚úÖ Model loaded on cuda\n",
            "\n",
            "üöÄ Launching...\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://cdf45c891a45bb69cf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFg9s7kRWI6I",
        "outputId": "1b4efa78-6e37-4587-ce8c-13f6ca2689ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (2.32.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.10.5)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0603c8530e49487ea08a62ab6f2c6d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df219834e198404d9556f92b2166a364",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c8c4c97676e4a55a1ad6214e6b18155",
            "value": 4
          }
        },
        "0c8c4c97676e4a55a1ad6214e6b18155": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10b41ad6c87642c082d8039d3d08fe9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22acb4513cef48d4852d1f1ae93d5215": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c35b42c8121c43129908ae58436a7b62",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_25cacd33f52f474da64e1c3480edc7fc",
            "value": "‚Äá4/4‚Äá[00:03&lt;00:00,‚Äá‚Äá1.29‚Äáexamples/s]"
          }
        },
        "250b0b1089574afdbb26e70178917fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64fae0c9a6ff4bd7b997931ca1620065",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_46815fd171f6491690d0fbffa5d19d99",
            "value": "Map:‚Äá100%"
          }
        },
        "25cacd33f52f474da64e1c3480edc7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2600caeb992346d5a184dd8c3ae141d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "267b4159b646403aadc194f96937050e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_846f0d16e25444928198678d4dd1109b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b43e4b6ffc6645d0bf857004b8cdad9b",
            "value": "Map:‚Äá100%"
          }
        },
        "2684a100a5b143e699647b0783b78974": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a94d7b3ca7dd4542ae76a3360c3057c4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_806ad090ff524e398f08a2746077c30f",
            "value": "‚Äá4/4‚Äá[00:03&lt;00:00,‚Äá‚Äá1.31‚Äáexamples/s]"
          }
        },
        "349b46601a3244debb5fc1778a9062d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_250b0b1089574afdbb26e70178917fa5",
              "IPY_MODEL_a4fcf85ccec340ad9fb51800bc88ac8a",
              "IPY_MODEL_22acb4513cef48d4852d1f1ae93d5215"
            ],
            "layout": "IPY_MODEL_abb51b60fe3f4dc6b30b1205d46eb004"
          }
        },
        "3a92b00ab0a54786bdd95813b6d5c4fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b54c33da1434476ba1535cfcfc8101b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41b1b62a45cd4600be107592188704f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe5a50a7103549e99f69a3adf9ad9288",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86ad791923f246f09cf2e0dd4a3acb6f",
            "value": 1
          }
        },
        "43ecc78c5dac4a0b94d808fe6ea6735f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bf0003d76e944488886ef420b39d675",
              "IPY_MODEL_41b1b62a45cd4600be107592188704f7",
              "IPY_MODEL_d531d9f9f08b4b48b92b502506b18a36"
            ],
            "layout": "IPY_MODEL_867b9b315f8e46b0949969136a8fb242"
          }
        },
        "46815fd171f6491690d0fbffa5d19d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59c49ad3173e4b20aa2018ce5923bc1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b928a8d77164cf09af1d71f6c0dc270": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85cd416cc10f42b7bb326d7d9b6fb6f3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_892b6f1614af4a12ad5764170eb8c8af",
            "value": "‚Äá12/12‚Äá[00:11&lt;00:00,‚Äá‚Äá1.08‚Äáexamples/s]"
          }
        },
        "60d9b7e61c544ea4996f5b905e26d062": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61c3a6d3dc5749b69bb0d0ebf61f9746": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64fae0c9a6ff4bd7b997931ca1620065": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd46a587b1e4a71a876a0538917a170": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e68160adef145dd837b4d7f2fe4e021": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8e1390693b4eaabda78ec4408eafcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_267b4159b646403aadc194f96937050e",
              "IPY_MODEL_f2a8f68baa2b41e0bfab543c20fe0a6b",
              "IPY_MODEL_5b928a8d77164cf09af1d71f6c0dc270"
            ],
            "layout": "IPY_MODEL_6e68160adef145dd837b4d7f2fe4e021"
          }
        },
        "7be2b8033d2e486392502af5e9cb1afa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "806ad090ff524e398f08a2746077c30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81af122f09b24610b02102dfe7944d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "846f0d16e25444928198678d4dd1109b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85cd416cc10f42b7bb326d7d9b6fb6f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "867b9b315f8e46b0949969136a8fb242": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ad791923f246f09cf2e0dd4a3acb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "892b6f1614af4a12ad5764170eb8c8af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b6d105924a54756970d20ca946371e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bf0003d76e944488886ef420b39d675": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7be2b8033d2e486392502af5e9cb1afa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ee1c8db8a77a4892a0d798a283d4dc7f",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "9c4f6708e550496a9c18c57847757be9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0a3c11ca39b4515b5f772b82354e6ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4fcf85ccec340ad9fb51800bc88ac8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59c49ad3173e4b20aa2018ce5923bc1e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad621c0eec8d4b2292d2c1a101ea534b",
            "value": 4
          }
        },
        "a94d7b3ca7dd4542ae76a3360c3057c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abb51b60fe3f4dc6b30b1205d46eb004": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad621c0eec8d4b2292d2c1a101ea534b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0f1e7e690a24cb79ebae8a9c8115843": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b76074a16cc94ae6bbd64e929d77ce87",
              "IPY_MODEL_0603c8530e49487ea08a62ab6f2c6d1a",
              "IPY_MODEL_2684a100a5b143e699647b0783b78974"
            ],
            "layout": "IPY_MODEL_9c4f6708e550496a9c18c57847757be9"
          }
        },
        "b3644da5401a44a69e92b783185e2768": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b43e4b6ffc6645d0bf857004b8cdad9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b76074a16cc94ae6bbd64e929d77ce87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61c3a6d3dc5749b69bb0d0ebf61f9746",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_10b41ad6c87642c082d8039d3d08fe9f",
            "value": "Map:‚Äá100%"
          }
        },
        "bcff0f34d34a4d89bc35662444f6c2b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35b42c8121c43129908ae58436a7b62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c86c938b130146afb19370d08fb4285c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f88c0f81c0e7449cb3ace76babd0bccf",
              "IPY_MODEL_d41a8dcb586a44ed962bdbd942c6ac5b",
              "IPY_MODEL_dd27452cf7a14c7381236044ee907689"
            ],
            "layout": "IPY_MODEL_3a92b00ab0a54786bdd95813b6d5c4fd"
          }
        },
        "d41a8dcb586a44ed962bdbd942c6ac5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b54c33da1434476ba1535cfcfc8101b",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0a3c11ca39b4515b5f772b82354e6ce",
            "value": 12
          }
        },
        "d531d9f9f08b4b48b92b502506b18a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60d9b7e61c544ea4996f5b905e26d062",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e02f11febfa542828f5fd0137aaa6ff5",
            "value": "‚Äá5.13k/?‚Äá[00:00&lt;00:00,‚Äá522kB/s]"
          }
        },
        "dd27452cf7a14c7381236044ee907689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd46a587b1e4a71a876a0538917a170",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_81af122f09b24610b02102dfe7944d5b",
            "value": "‚Äá12/12‚Äá[00:12&lt;00:00,‚Äá‚Äá1.24‚Äáexamples/s]"
          }
        },
        "df219834e198404d9556f92b2166a364": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02f11febfa542828f5fd0137aaa6ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee1c8db8a77a4892a0d798a283d4dc7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2a8f68baa2b41e0bfab543c20fe0a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b6d105924a54756970d20ca946371e2",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2600caeb992346d5a184dd8c3ae141d6",
            "value": 12
          }
        },
        "f88c0f81c0e7449cb3ace76babd0bccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcff0f34d34a4d89bc35662444f6c2b1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b3644da5401a44a69e92b783185e2768",
            "value": "Map:‚Äá100%"
          }
        },
        "fe5a50a7103549e99f69a3adf9ad9288": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}