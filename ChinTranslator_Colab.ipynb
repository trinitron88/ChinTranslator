{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca2174e",
   "metadata": {},
   "source": [
    "# Hakha Chin Whisper Fine-Tuning (Colab)\n",
    "\n",
    "This notebook is configured to run entirely on Google Colab. It mounts Google Drive, installs dependencies, prepares datasets, fine-tunes Whisper on Hakha Chin, and saves outputs back to Drive.\n",
    "\n",
    "Requirements:\n",
    "- Google Colab GPU runtime preferred\n",
    "- Your project folder in Drive: /content/drive/MyDrive/ChinTranslator\n",
    "- Expected files (if using aligned segments): aligned_train_data.json, aligned_val_data.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aaf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Colab Runtime\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if not IN_COLAB:\n",
    "    print(\"âš ï¸ Not running in Google Colab. This notebook is intended for Colab execution.\")\n",
    "else:\n",
    "    print(\"âœ… Running in Google Colab.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a300840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify and Configure Accelerator\n",
    "import subprocess, sys\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Device: {device}\")\n",
    "    if device == 'cuda':\n",
    "        try:\n",
    "            # Show GPU info\n",
    "            print(subprocess.run(['nvidia-smi'], check=False, capture_output=True, text=True).stdout)\n",
    "            print(f\"Torch CUDA: {torch.version.cuda}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        print(\"âš ï¸ GPU not detected. Training will be slow on CPU.\")\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not yet installed; will install in the next step.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794288a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "%pip -q install transformers datasets accelerate evaluate jiwer librosa soundfile sentencepiece torchaudio tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b04fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate and Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Skipping Drive mount (not in Colab)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8687f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or Update Repository from GitHub (optional)\n",
    "import os, subprocess\n",
    "REPO_URL = 'https://github.com/trinitron83/ChinTranslator2'\n",
    "WORK_DIR = '/content/ChinTranslator2'\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists(WORK_DIR):\n",
    "        !git clone $REPO_URL $WORK_DIR\n",
    "    else:\n",
    "        %cd $WORK_DIR\n",
    "        !git pull --rebase\n",
    "    %cd /content\n",
    "else:\n",
    "    print(\"Skipping git clone/update (not in Colab)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Project Paths and Environment Variables\n",
    "import sys\n",
    "ROOT_DIR = '/content/drive/MyDrive/ChinTranslator'\n",
    "DATA_DIR = f'{ROOT_DIR}'\n",
    "OUTPUT_DIR = f'{ROOT_DIR}/whisper-hakha-chin-v3'\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Add repo to Python path if cloned\n",
    "if IN_COLAB and os.path.exists('/content/ChinTranslator2'):\n",
    "    sys.path.append('/content/ChinTranslator2')\n",
    "\n",
    "print('ROOT_DIR:', ROOT_DIR)\n",
    "print('OUTPUT_DIR:', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Secrets and Config (optional)\n",
    "from getpass import getpass\n",
    "\n",
    "# Example: Hugging Face token (not required for public models)\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN', '')\n",
    "if not HF_TOKEN:\n",
    "    print('No HF token set; proceeding with public access.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbe7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Unit Tests with Pytest (optional)\n",
    "if IN_COLAB and os.path.exists('/content/ChinTranslator2/tests'):\n",
    "    %cd /content/ChinTranslator2\n",
    "    !pytest -q || true\n",
    "    %cd /content\n",
    "else:\n",
    "    print('No tests found or not in Colab; skipping pytest.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download or Cache Data (verify presence of aligned files)\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "aligned_train = Path(f\"{ROOT_DIR}/aligned_train_data.json\")\n",
    "aligned_val = Path(f\"{ROOT_DIR}/aligned_val_data.json\")\n",
    "training_full = Path(f\"{ROOT_DIR}/training_data.json\")\n",
    "\n",
    "print('Looking for:', aligned_train, aligned_val)\n",
    "print('Exists:', aligned_train.exists(), aligned_val.exists())\n",
    "\n",
    "# No external download needed; files are expected in Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b092975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Project Entry Points: Prepare datasets\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "# Load data from JSON files\n",
    "if aligned_train.exists() and aligned_val.exists():\n",
    "    with open(aligned_train, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(aligned_val, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    print(f\"Loaded aligned segments: train={len(train_data)}, val={len(val_data)}\")\n",
    "elif training_full.exists():\n",
    "    # Fallback: split full training_data.json 80/20\n",
    "    with open(training_full, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    n = len(data)\n",
    "    split = int(0.8 * n)\n",
    "    train_data = data[:split]\n",
    "    val_data = data[split:]\n",
    "    print(f\"Loaded full dataset and split: train={len(train_data)}, val={len(val_data)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No aligned or full training data JSON files found in Drive.\")\n",
    "\n",
    "# Normalize audio paths\n",
    "from pathlib import PurePosixPath\n",
    "\n",
    "missing = 0\n",
    "for lst in (train_data, val_data):\n",
    "    for item in lst:\n",
    "        p = item['audio']\n",
    "        # If path is relative, prefix with ROOT_DIR\n",
    "        if not p.startswith('/'):\n",
    "            p = f\"{ROOT_DIR}/{p}\"\n",
    "        # If path points to local repo-like path, ensure it uses ROOT_DIR\n",
    "        if '/ChinTranslator/' in p and not p.startswith('/content/drive/'):\n",
    "            # Replace up to 'ChinTranslator' with ROOT_DIR\n",
    "            idx = p.rfind('/ChinTranslator')\n",
    "            p = f\"{ROOT_DIR}{p[idx+len('/ChinTranslator'):] }\"\n",
    "        item['audio'] = p\n",
    "        if not os.path.exists(p):\n",
    "            missing += 1\n",
    "\n",
    "print(f\"Missing audio files (will be skipped at load time): {missing}\")\n",
    "\n",
    "# Build HF Datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'audio': [x['audio'] for x in train_data],\n",
    "    'text': [x['text'] for x in train_data],\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'audio': [x['audio'] for x in val_data],\n",
    "    'text': [x['text'] for x in val_data],\n",
    "})\n",
    "\n",
    "train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate=16000))\n",
    "val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate=16000))\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper model and processor\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    ")\n",
    "\n",
    "MODEL_NAME = 'openai/whisper-small'\n",
    "TASK = 'transcribe'\n",
    "\n",
    "from pathlib import Path\n",
    "prev_model_dir = Path(f\"{ROOT_DIR}/whisper-hakha-chin-v2\")\n",
    "if prev_model_dir.exists():\n",
    "    print(f\"Continuing training from: {prev_model_dir}\")\n",
    "    processor = WhisperProcessor.from_pretrained(str(prev_model_dir), language=None, task=TASK)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(str(prev_model_dir))\n",
    "else:\n",
    "    print(f\"Loading base model: {MODEL_NAME}\")\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=None, task=TASK)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Recommended project settings\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False  # disable for training\n",
    "\n",
    "# Explicitly disable gradient checkpointing due to prior issues in project notes\n",
    "try:\n",
    "    model.gradient_checkpointing_disable()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print('âœ… Model and processor ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ea9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data mapping\n",
    "\n",
    "def prepare_data(batch):\n",
    "    audio = batch['audio']\n",
    "    out = processor.feature_extractor(\n",
    "        audio['array'], sampling_rate=audio['sampling_rate']\n",
    "    )\n",
    "    batch['input_features'] = out.input_features[0]\n",
    "    batch['labels'] = processor.tokenizer(batch['text']).input_ids\n",
    "    return batch\n",
    "\n",
    "print('Mapping datasets...')\n",
    "train_dataset_proc = train_dataset.map(prepare_data, remove_columns=train_dataset.column_names)\n",
    "val_dataset_proc = val_dataset.map(prepare_data, remove_columns=val_dataset.column_names)\n",
    "print('âœ… Datasets processed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab017e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if labels.shape[1] > 0 and (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "print('âœ… Collator ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "import evaluate\n",
    "metric = evaluate.load('wer')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {'wer': wer}\n",
    "\n",
    "print('âœ… Metrics ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd81b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=15,\n",
    "    gradient_checkpointing=False,\n",
    "    fp16=True if (torch.cuda.is_available()) else False,\n",
    "    evaluation_strategy='steps',\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    logging_steps=50,\n",
    "    report_to=['tensorboard'],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='wer',\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "print(training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback\n",
    "\n",
    "class ProgressCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            print(f\"Step {state.global_step}: {logs}\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset_proc,\n",
    "    eval_dataset=val_dataset_proc,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[ProgressCallback()],\n",
    ")\n",
    "\n",
    "print('âœ… Trainer ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f04977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "print('\\nðŸš€ Starting training...')\n",
    "train_result = trainer.train()\n",
    "print('\\nâœ… Training complete!')\n",
    "train_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist Artifacts to Drive\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Saved model and processor to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Inference Test\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import librosa\n",
    "\n",
    "print('Loading model for inference...')\n",
    "proc = WhisperProcessor.from_pretrained(OUTPUT_DIR)\n",
    "mdl = WhisperForConditionalGeneration.from_pretrained(OUTPUT_DIR)\n",
    "mdl.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sample_audio = train_data[0]['audio'] if len(train_data) else None\n",
    "print('Sample:', sample_audio)\n",
    "if sample_audio and os.path.exists(sample_audio):\n",
    "    audio, _ = librosa.load(sample_audio, sr=16000)\n",
    "    inputs = proc(audio, sampling_rate=16000, return_tensors='pt').input_features\n",
    "    inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gen_ids = mdl.generate(inputs, max_length=200, repetition_penalty=1.5, no_repeat_ngram_size=3)\n",
    "    text = proc.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "    print('Transcription:\\n', text)\n",
    "else:\n",
    "    print('No valid sample audio found for quick test.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140a0cb",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Ensure aligned JSONs are in `/content/drive/MyDrive/ChinTranslator`.\n",
    "- Consider continuing from `whisper-hakha-chin-v2` if present.\n",
    "- Avoid enabling gradient checkpointing due to prior PyTorch issues.\n",
    "- Prefer file uploads over mic recordings unless you normalize audio to 16kHz mono.\n",
    "- Scale data (more chapters, diverse speakers) for better generalization.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
